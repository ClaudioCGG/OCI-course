# <center>üåê Bienvenido a Ciencia de Datos</center>

## üìö √çndice tem√°tico del curso OCI Data Science 

### 1. **Bienvenida e Introducci√≥n General**
- Qu√© es Data Science y por qu√© es relevante hoy
- Objetivo del curso y habilidades requeridas
- Casos de uso: Employee Attrition
- Estructura del curso por m√≥dulos
- Consejos de estudio y comunidad de aprendizaje

### 2. **Historia y Evoluci√≥n de Data Science**
- Filosof√≠a: Navaja de Ockham
- Primeros enfoques cient√≠ficos (Mayer, Tukey, Samuel)
- Hitos tecnol√≥gicos (Deep Blue, AutoML)
- Nacimiento del t√©rmino ‚ÄúData Science‚Äù
- Aplicaciones modernas: Great Resignation y predicci√≥n de abandono

### 3. **Servicios de Oracle para Data Science y AI**
- Diferencias entre ML Services y AI Services
- OCI Data Labeling y Cat√°logo de Modelos
- Arquitectura: datos, aplicaciones, servicios
- Principios gu√≠a: aceleraci√≥n individual, colaboraci√≥n y escalabilidad empresarial

### 4. **Terminolog√≠a y Componentes Clave de OCI Data Science**
- Projects, Notebook Sessions, Conda
- ADS SDK: funcionalidades y flujo completo
- Model Catalog, Model Deployment, Data Science Jobs
- Interfaces disponibles: consola, SDK, CLI, API REST

### 5. **Visualizaci√≥n, Ingenier√≠a de Features y Entrenamiento**
- Acceso a datos: local, object storage, bases Oracle y terceros
- Exploraci√≥n de datos: Feature Types, heatmaps, estad√≠sticas
- Feature Engineering: codificaci√≥n, imputaci√≥n, recomendaciones
- Entrenamiento de modelos: AutoML, ADSTuner
- Evaluaci√≥n de modelos y explicaci√≥n (interpretabilidad local/global)

### 6. **Pol√≠ticas, Tenencia y Configuraci√≥n IAM**
- Compartments, grupos de usuarios, grupos din√°micos
- Sintaxis de pol√≠ticas: verbs, resource types
- Pol√≠ticas requeridas y √∫tiles para logging, m√©tricas, red y servicios relacionados
- Ejemplo de configuraci√≥n manual y con OCI Resource Manager
- Acceso y estructura del Terraform Script

### 7. **Networking para Data Science**
- Componentes de red: VCN, Subnets, VNICs, DRG, NAT, Service Gateway
- Patrones: red por defecto vs red personalizada
- Uso del VCN Wizard
- Requisitos para conectar workloads a activos externos

### 8. **Autenticaci√≥n en APIs de OCI**
- Interfaces disponibles: ADS SDK, Python SDK, CLI
- Diferencias entre autenticaci√≥n y autorizaci√≥n
- Resource Principals: seguridad, rotaci√≥n de certificados
- Archivos de configuraci√≥n y claves API (.pem)
- Notebooks de ejemplo para crear credenciales</br></br>

************************************************************************************************************
</br>
</br>



### -- Visi√≥n General del Curso

Bienvenidos todos al curso de Ciencia de Datos de Oracle Cloud Infrastructure. La ciencia de datos es el arte y la ciencia de extraer conocimientos valiosos de los datos para resolver problemas del mundo real y de negocios.

Y este es el momento perfecto para mejorar o reentrenar a tu fuerza laboral para satisfacer esta enorme demanda de profesionales en ciencia de datos. Estamos emocionados de tenerte aqu√≠ y hemos preparado una gran cantidad de informaci√≥n valiosa para ti. Para simplificar, usar√© OCI para referirme a Oracle Cloud Infrastructure, que es nuestra poderosa plataforma.

En las siguientes diapositivas, te guiar√© a trav√©s del p√∫blico objetivo, los requisitos previos, los objetivos y la estructura del curso. Pero antes de eso, echemos un vistazo a las personas que contribuyeron a este curso.

Adem√°s de m√≠, escuchar√°s a varios expertos y especialistas a lo largo de este curso, incluyendo Wes Prichard, John Peach, John Stanesby, JR Gauthier, Lyudmil Pelov, Praveen Patil y Hemant Gahankari. Aunque tal vez no escuches sus voces, hay docenas de personas que ayudaron a desarrollar este curso. Est√°n listadas aqu√≠ alfab√©ticamente por equipo.

Este curso fue dise√±ado con una audiencia espec√≠fica en mente. Est√° destinado principalmente a cient√≠ficos de datos, pero tambi√©n es apropiado para roles relacionados como ingenieros de aprendizaje autom√°tico (ML engineers) e ingenieros de inteligencia artificial (AI engineers).

Nuestro objetivo es ayudarte a dominar el uso de OCI Data Science y servicios en la nube relacionados para construir soluciones de ciencia de datos. Para eso, deber√≠as tener habilidades en Python para aprendizaje autom√°tico. Deber√≠as tener conocimientos generales de bibliotecas de ciencia de datos y aprendizaje autom√°tico de c√≥digo abierto, y saber c√≥mo aplicarlas.

Eso significa que probablemente tienes un a√±o o m√°s de experiencia en uno de los roles mencionados en la diapositiva anterior. Y ser√° √∫til si ya tienes algo de experiencia pr√°ctica con OCI.

Dado que este curso trata sobre ayudarte a prepararte para el examen de certificaci√≥n, es √∫til saber desde el principio lo que el examen va a validar. Evaluar√° tu capacidad para identificar servicios de OCI utilizados para implementar una soluci√≥n de aprendizaje autom√°tico para un caso de uso empresarial. Verificar√° si puedes incorporar buenas pr√°cticas de aprendizaje autom√°tico en la nube.

Se centrar√° fuertemente en el uso de OCI Data Science para construir, entrenar, desplegar y gestionar modelos de ML. Tambi√©n incluir√° el uso de otros servicios de datos e inteligencia artificial de OCI para crear soluciones de aprendizaje autom√°tico.

Este curso est√° dividido en cinco m√≥dulos principales. Tomemos un momento para repasarlos. Una introducci√≥n a la ciencia de datos presentar√° OCI Data Science, y c√≥mo configurar una tenencia de OCI (OCI tenancy) para usar OCI Data Science. La configuraci√≥n del entorno se centrar√° en la preparaci√≥n del entorno de OCI Data Science.

El m√≥dulo del ciclo de vida de aprendizaje autom√°tico te guiar√° por las capacidades de OCI Data Science que apoyan todos los pasos del ciclo de vida de ML. Las pr√°cticas de MLOps se enfocan en caracter√≠sticas que soportan MLOps como escalado, monitoreo.

Y finalmente, Servicios Relacionados de OCI cubre otros servicios en la nube √∫tiles al construir soluciones de ciencia de datos. Cada uno de estos m√≥dulos contiene m√∫ltiples lecciones presentadas por diferentes especialistas. Recomendamos avanzar en los m√≥dulos en orden, porque los m√≥dulos posteriores se basan en los anteriores.

Muchas de las lecciones incluir√°n una demostraci√≥n grabada para ilustrar los conceptos y pr√°cticas importantes presentadas en la lecci√≥n. El curso tambi√©n incluye un laboratorio de extremo a extremo que los estudiantes pueden usar para obtener experiencia pr√°ctica y reforzar conceptos de todos los m√≥dulos.

Este laboratorio utiliza un caso de uso sobre la deserci√≥n de empleados (employee attrition). Y muchas de las demostraciones usar√°n ese mismo caso. El caso predice la probabilidad de que un empleado deje la organizaci√≥n seg√∫n m√∫ltiples caracter√≠sticas.

Para completar el laboratorio, necesitar√°s acceso a una cuenta de Oracle Cloud. Si a√∫n no tienes una, puedes registrarte para una prueba gratuita en signup.cloud.oracle.com. Tambi√©n puedes usar GitHub para acceder al repositorio de ejemplos de OCI Data Science y AI (OCI Data Science AI samples repo). Y vamos a presentarte la terminolog√≠a de los productos de ciencia de datos en el primer m√≥dulo.

En cualquier momento, si tienes una pregunta espec√≠fica sobre el material del curso o necesitas ayuda adicional, completa nuestro formulario ‚ÄúAsk Your Instructor‚Äù. Nuestros instructores expertos se comunicar√°n contigo lo antes posible con soporte personalizado.

Tambi√©n queremos que saques el m√°ximo provecho de tu experiencia de aprendizaje. Por eso hemos creado este espacio comunitario donde puedes conectar con otros estudiantes y expertos en la materia. Si tienes alguna otra pregunta o quieres iniciar una discusi√≥n sobre un tema en particular, este es el lugar. As√≠ que no seas t√≠mido. √önete hoy a la comunidad de Oracle University (OU Community) y empieza a colaborar con tus compa√±eros de estudio. Estamos ansiosos por ver lo que aportar√°s.

Me gustar√≠a reconocer que este es un curso extenso. Tambi√©n me gustar√≠a ofrecer algunos consejos para mejorar la retenci√≥n y darte la mejor oportunidad de aprobar el examen. Sugerimos que tomes notas sobre los temas seg√∫n tu conocimiento previo. Y recuerda que puedes seguir usando la transcripci√≥n de preparaci√≥n.

Programa descansos cada hora y mu√©vete. No permanezcas est√°tico frente a la computadora por mucho tiempo. Reg√≠strate para una cuenta gratuita en la nube. Familiar√≠zate con la plataforma OCI y completa todos los ejercicios de verificaci√≥n de habilidades en el curso. Tambi√©n completa la preparaci√≥n para el examen y toma el examen de pr√°ctica antes de presentar el examen de certificaci√≥n.

Aqu√≠ en OCI estamos continuamente creando y entregando formaci√≥n, integrando comentarios y monitoreando los an√°lisis de uso. Si algo est√° roto o no est√° resonando con nuestros usuarios, queremos saberlo‚Äîy ah√≠ es donde entran tus valoraciones.

Si√©ntete libre de calificar este curso y dejar comentarios espec√≠ficos sobre lo que fue √∫til y lo que no. Estamos ajustando constantemente nuestro enfoque para ayudar a nuestra audiencia a lograr sus metas. As√≠ que unamos fuerzas y trabajemos juntos para tu aprendizaje y certificaci√≥n.</br></br>

---

### -- Consejos de Expertos: Introducci√≥n

Primero que nada, gracias por elegir tomar el curso profesional de OCI Data Science y obtener la certificaci√≥n. Mi nombre es Hemant Gahankari. Soy l√≠der principal de formaci√≥n en Oracle University.

Como cient√≠fico de datos o ingeniero de aprendizaje autom√°tico, nuestro trabajo diario consiste en obtener datos, preparar datos, construir y entrenar modelos, evaluar modelos, desplegar y escalar modelos, y tambi√©n automatizar pipelines (flujos de trabajo) de aprendizaje autom√°tico. Con el servicio OCI Data Science y los servicios de IA, podemos realizar todas estas tareas de manera eficiente.

A trav√©s de una serie de videos con consejos de expertos, te mostraremos c√≥mo usar algunas de las caracter√≠sticas poderosas ‚Äîy al mismo tiempo f√°ciles de usar‚Äî del servicio de ciencia de datos y servicios de IA de OCI. Esperamos que estos videos te resulten √∫tiles. Gracias por escuchar.</br></br>

---
### -- Introducci√≥n y Configuraci√≥n

#### -- Ciencia de Datos: Introducci√≥n

Este es el m√≥dulo 1, que cubre introducci√≥n y configuraci√≥n. Esta primera lecci√≥n es la introducci√≥n al servicio de Ciencia de Datos en la Nube de Oracle Cloud Infrastructure. Soy Wes Pritchard, gerente principal de producto para Ciencia de Datos y Servicios de IA.

---

## üìú Antes de entrar en ciencia de datos y Oracle

Echemos una mirada divertida a la historia y c√≥mo llegamos aqu√≠. En los a√±os 1300, William Ockham, un fil√≥sofo y fraile, cre√≠a que los cient√≠ficos deber√≠an preferir teor√≠as m√°s simples por sobre las m√°s complejas. El principio que lleva su nombre, conocido como la navaja de Ockham (*Ockham‚Äôs razor*), puede aplicarse al aprendizaje autom√°tico buscando la soluci√≥n m√°s simple.

A mediados de 1700, el astr√≥nomo Tobias Mayer hizo un argumento cuantitativo de que m√°s datos son mejores. Estudiaba los movimientos de la luna y recolect√≥ 9 veces m√°s puntos de datos de los necesarios, afirmando que esto hac√≠a sus observaciones m√°s precisas. Por esto, a menudo se le considera el primer verdadero cient√≠fico de datos.

En 1952, Arthur Samuel, pionero de IBM en computaci√≥n, juegos e inteligencia artificial, acu√±√≥ el t√©rmino *machine learning* (aprendizaje autom√°tico). Dise√±√≥ un juego para jugar damas y descubri√≥ que cuanto m√°s jugaba la computadora, m√°s aprend√≠a estrategias ganadoras mediante la experiencia.

En 1962, el matem√°tico John W. Tukey predijo el efecto de la computaci√≥n electr√≥nica moderna en el an√°lisis de datos como una ciencia emp√≠rica. Sin embargo, sus predicciones ocurrieron d√©cadas antes de la explosi√≥n del *big data* y de la capacidad para realizar an√°lisis complejos y a gran escala.

En 1997, una supercomputadora de IBM llamada Deep Blue derrot√≥ al gran maestro de ajedrez Garry Kasparov en solo 19 movimientos. Kasparov se rindi√≥ tras este partido. La supercomputadora, altamente avanzada, pod√≠a calcular entre 100 mil millones y 200 mil millones de posiciones en los tres minutos tradicionalmente asignados a cada jugador por jugada en ajedrez est√°ndar.

En 2008, el Dr. DJ Patil de LinkedIn y Jeff Hammerbacher de Facebook acu√±aron el t√©rmino *data science* (ciencia de datos) para describir un campo de estudio emergente que se enfocaba en extraer el valor oculto de los datos recolectados en los sectores comerciales y minoristas.

Dado ese contexto hist√≥rico, veamos c√≥mo se aplica hoy la ciencia de datos. En 2021, en medio de la pandemia global por COVID-19, el profesor y psic√≥logo Anthony Klotz acu√±√≥ el t√©rmino *la Gran Renuncia* (*the Great Resignation*) para describir una nueva tendencia de insatisfacci√≥n y rotaci√≥n en el empleo. Muchas empresas quieren rastrear, analizar y predecir los patrones de retenci√≥n de sus empleados.

En este curso, usaremos la deserci√≥n de empleados (*employee attrition*) como un caso de uso para conectar nuestras actividades de aprendizaje autom√°tico con un problema empresarial del mundo real. Mejor a√∫n, te ayudaremos a construir un modelo predictivo de ML (*Machine Learning*) por ti mismo en el laboratorio independiente para estudiantes que acompa√±a este curso.</br></br>

---

## üîç Enfoque de Oracle hacia ciencia de datos e IA

Todo gira en torno a los datos. Durante muchos a√±os, los datos disponibles para las organizaciones eran los datos estructurados de sus aplicaciones empresariales. Y estos siguen siendo datos de negocios muy importantes, pero ciertamente no son los √∫nicos. Las organizaciones tienen muchos tipos de datos √∫nicos y a menudo no estructurados, provenientes de muchas fuentes diferentes: sensores de equipos, aplicaciones m√≥viles, redes sociales, interacciones con clientes v√≠a voz y texto, videos, im√°genes, documentos, y muchos m√°s.

Las organizaciones quieren usar **todos los datos** para producir nuevos conocimientos y nuevos productos de datos. Quieren mejorar sus operaciones creando mejores experiencias para clientes, anticipando demanda de servicios y evitando fallas de equipos que se podr√≠an haber prevenido. La siguiente generaci√≥n de problemas empresariales o escenarios exige poder usar todos los datos. Y necesitamos las capacidades que brindan la ciencia de datos, el aprendizaje autom√°tico y la inteligencia artificial para comprender y utilizar esos datos.

Oracle AI es el portafolio de servicios en la nube para ayudar a las organizaciones a aprovechar todos los datos en esta nueva generaci√≥n de escenarios. Por lo tanto, la **base de todo esto son los datos**. Es como una barra en la parte inferior. Obviamente, la IA y el aprendizaje autom√°tico trabajan sobre datos y requieren datos.

Ahora, la capa superior de este diagrama son las aplicaciones, y esto se refiere de forma amplia a todas las maneras en que se consume la IA. Puede ser una aplicaci√≥n, un proceso de negocio o un sistema anal√≠tico.

Entre la capa de aplicaciones y la de datos, hay dos grupos: los servicios de IA (*AI services*) en la parte superior y los servicios de aprendizaje autom√°tico (*ML services*) en la parte inferior. La diferencia entre los dos grupos es que los servicios de ML son utilizados principalmente por los cient√≠ficos de datos para construir, entrenar, desplegar y administrar modelos de ML. Los cient√≠ficos de datos pueden trabajar con frameworks de c√≥digo abierto conocidos y con OCI Data Science. Por cierto, ese es el servicio en la nube que es el enfoque de este curso.

Los cient√≠ficos de datos y especialistas en bases de datos pueden aprovechar algoritmos de ML incorporados en la base de datos de Oracle. Y un servicio importante que respalda tanto a los servicios de ML como de IA es **OCI Data Labeling** (etiquetado de datos), ya que al construir modelos de ML que trabajan con im√°genes, texto o voz, se necesita **datos etiquetados** para entrenar los modelos.

Los servicios de IA contienen modelos de ML preconstruidos para usos espec√≠ficos. Algunos est√°n preentrenados y otros son entrenados por el cliente con sus propios datos. Todos se utilizan simplemente llamando a la API del servicio, enviando los datos a procesar, y el servicio devuelve un resultado. No hay infraestructura que administrar. Y aunque este curso se centrar√° principalmente en OCI Data Science, tambi√©n incluimos algunas lecciones sobre servicios de IA y etiquetado de datos. Hay otro curso aparte que cubre ML y base de datos Oracle.

Ahora bien, esos servicios de IA y ML que acabo de mostrarte no funcionan de manera aislada. Est√°n respaldados por muchos otros servicios disponibles en nuestra infraestructura en la nube, incluyendo an√°lisis de negocios, an√°lisis de grafos y muchas formas de integraci√≥n y gesti√≥n de datos, todo funcionando sobre la infraestructura b√°sica de la nube. Estos servicios pueden combinarse en varias arquitecturas para respaldar diferentes escenarios.</br></br>

---

## üîß OCI Data Science: visi√≥n general

Ya definimos Oracle AI y los servicios que lo componen. Ahora, veamos m√°s de cerca Oracle Cloud Infrastructure Data Science (abreviado como OCI Data Science). Es el servicio en la nube enfocado en asistir al cient√≠fico de datos durante todo el ciclo de vida de aprendizaje autom√°tico, con soporte para Python y software de c√≥digo abierto. Como ver√°s en los √≠conos del gr√°fico, el servicio tiene muchas caracter√≠sticas que cubriremos a lo largo del curso.

Ahora, repasemos los tres principios fundamentales que gu√≠an el producto:

1. **Acelerar el trabajo del cient√≠fico de datos individual**. Quienes salen hoy de las universidades han sido formados usando herramientas de c√≥digo abierto, y eso es lo que les resulta m√°s c√≥modo. Pero usar estas herramientas en una laptop significa gestionar muchas bibliotecas de diferentes fuentes, y estar limitado por la potencia de c√≥mputo de la m√°quina.

   OCI Data Science proporciona bibliotecas de c√≥digo abierto junto con acceso f√°cil a diferentes niveles de potencia de c√≥mputo sin necesidad de gestionar ninguna infraestructura. Tambi√©n incluye una biblioteca propia de Oracle para facilitar varios aspectos del trabajo del cient√≠fico de datos.

2. **Colaboraci√≥n**. Va m√°s all√° de la productividad individual, permitiendo que los equipos de ciencia de datos trabajen juntos. Esto se logra mediante el **compartir recursos**, lo que reduce el trabajo duplicado y apoya la **reproducibilidad y auditabilidad** de los modelos, para facilitar la colaboraci√≥n y la gesti√≥n de riesgos.

3. **Calidad empresarial (*Enterprise-grade*)**.El tercer principio trata sobre ser de calidad empresarial. Eso significa que est√° integrado con todos los protocolos de seguridad y acceso de OCI. La infraestructura subyacente est√° completamente gestionada. El cliente no tiene que pensar en aprovisionar c√≥mputo ni almacenamiento, ya que el servicio se encarga de todo el mantenimiento, actualizaciones y parches, para que los usuarios puedan enfocarse en resolver problemas empresariales con ciencia de datos.</br></br>

---

## ‚öôÔ∏è Detalles espec√≠ficos de OCI Data Science

Primero que nada, es un servicio en la nube para construir, entrenar, desplegar y gestionar modelos de aprendizaje autom√°tico de manera r√°pida. Sirve a cient√≠ficos de datos y equipos de ciencia de datos a lo largo del ciclo completo de vida del aprendizaje autom√°tico, con soporte para Python y herramientas de c√≥digo abierto. Los usuarios trabajan en una interfaz familiar de JupyterLab, donde escriben c√≥digo Python. Y los modelos se preservan en el cat√°logo de modelos (*Model Catalog*) y se despliegan para gestionar la infraestructura.</br></br>

---

## üß† Terminolog√≠a importante del producto

Vamos a cubrir algunos t√©rminos clave que se usar√°n a lo largo del curso. T√≥mate un tiempo para asimilarlos:

- **Projects (Proyectos)**: Son contenedores que permiten a los equipos de ciencia de datos organizar su trabajo. Representan espacios de colaboraci√≥n para organizar y documentar recursos como sesiones de notebook y modelos. Una tenencia (*tenancy*) puede tener tantos proyectos como se necesite, sin l√≠mites.

- **Notebook Sessions (Sesiones de Notebook)**: Es donde trabajan los cient√≠ficos de datos. Proveen un entorno de JupyterLab con bibliotecas de c√≥digo abierto preinstaladas y la posibilidad de agregar m√°s. Son entornos interactivos para codificar, construir y entrenar modelos. Estas sesiones corren sobre infraestructura gestionada, y el usuario puede seleccionar CPU o GPU, el tipo de c√≥mputo (*compute shape*) y la cantidad de almacenamiento sin necesidad de aprovisionar manualmente.

- **Conda**: Sistema de gesti√≥n de entornos y paquetes de c√≥digo abierto, creado para programas en Python. Se utiliza en el servicio de ciencia de datos para instalar, ejecutar y actualizar paquetes con sus dependencias r√°pidamente. Conda permite crear, guardar, cargar y alternar entre entornos de forma sencilla dentro del notebook.

- **ADS SDK (Accelerated Data Science Software Development Kit)**: Es una biblioteca en Python incluida en OCI Data Science. Tiene muchas funciones y objetos que automatizan o simplifican pasos del flujo de trabajo en ciencia de datos: conexi√≥n a datos, exploraci√≥n, visualizaci√≥n, entrenamiento con AutoML, evaluaci√≥n y explicaci√≥n de modelos. Adem√°s, ofrece una interfaz sencilla para acceder al cat√°logo de modelos y otros servicios de OCI, incluyendo almacenamiento de objetos (*Object Storage*).

---

## üîç Modelos, Cat√°logo y Despliegue

- **Modelos**: Definen una representaci√≥n matem√°tica de tus datos y negocio. Se crean en sesiones de notebook dentro de proyectos.

- **Model Catalog (Cat√°logo de Modelos)**: Lugar donde se almacenan, rastrean, comparten y gestionan los modelos. Es un repositorio centralizado y gestionado de artefactos de modelos. Incluye metadatos sobre el origen del modelo, informaci√≥n relacionada con Git, y el script o notebook usado para subirlo al cat√°logo. Los modelos almacenados pueden compartirse entre miembros del equipo y volverse a cargar en una sesi√≥n de notebook.

- **Model Deployments (Despliegue de Modelos)**: Permite desplegar modelos desde el cat√°logo como endpoints HTTP sobre infraestructura gestionada. Este tipo de despliegue como aplicaciones web que sirven predicciones en tiempo real es la forma m√°s com√∫n de operacionalizar modelos. Los endpoints HTTP son flexibles y pueden procesar solicitudes de predicci√≥n.

---

## üöÄ Tareas, Accesos y Regiones

- **Data Science Jobs (Tareas de Ciencia de Datos)**: Permiten definir y ejecutar tareas repetibles de aprendizaje autom√°tico en infraestructura gestionada.

- **OCI Console**: M√©todo m√°s com√∫n de acceso. Proporciona una interfaz basada en navegador, f√°cil de usar, que da acceso a las sesiones de notebook y todas las caracter√≠sticas del servicio. Esta ser√° la interfaz usada durante el curso.

- **REST API**: Proporciona acceso program√°tico a las funcionalidades del servicio. Hay documentaci√≥n con referencia de la API.

- **SDKs (Kits de desarrollo)**: OCI ofrece SDKs para varios lenguajes de programaci√≥n (Java, Python, TypeScript, JavaScript, .NET, Go, Ruby). Estos permiten escribir c√≥digo para gestionar recursos del servicio. Se mostrar√°n ejemplos del uso del SDK de Python para desplegar modelos y crear tareas.

- **CLI (Command Line Interface)**: Ofrece acceso r√°pido y funcionalidad completa sin necesidad de scripting.

- **Regions (Regiones)**: OCI Data Science como servicio en la nube est√° disponible a trav√©s de regiones, que son centros de datos distribuidos globalmente, ofreciendo entornos seguros y de alto rendimiento local. Esto lo hace accesible en todo el mundo para sectores comerciales, gubernamentales y dedicados. Oracle agrega nuevas regiones frecuentemente, m√°s informaci√≥n en [oracle.com/cloud](https://oracle.com/cloud).

---

## üìö ADS SDK Overview

Hola, bienvenido a este m√≥dulo sobre el SDK de Accelerated Data Science. Soy John Peach, cient√≠fico de datos en el equipo de OCI Data Science Service.

En este m√≥dulo obtendr√°s una comprensi√≥n general del ADS SDK, sus metas y capacidades. ADS SDK fue dise√±ado por y para cient√≠ficos de datos. Cubre todo el ciclo de vida del aprendizaje autom√°tico, con el objetivo de integrar los servicios de OCI en flujos de trabajo t√≠picos de ciencia de datos.

Por ejemplo, se integra con Autonomous Database y el servicio de Big Data mediante clases como *SecretKeeper*, que facilitan el almacenamiento seguro de credenciales y el acceso a esos servicios.

Tambi√©n busca mejorar tareas comunes como an√°lisis exploratorio con *Feature Types* (tipos de caracter√≠sticas), y optimizaci√≥n de hiperpar√°metros mediante *ADSTuner*. Adem√°s, ADS ofrece AutoML y funcionalidades de *explainability* (explicabilidad de modelos).

Hay dos versiones de ADS:

- Una p√∫blica disponible en GitHub o instalable desde PyPI.
- Otra especial que viene incluida en ciertos packs dentro del servicio de Oracle Cloud, que contiene las funcionalidades de AutoML y explicabilidad.

Se puede acceder al SDK de varias formas. Est√° instalado en los entornos *Conda* dentro del servicio de Data Science, listo para usarse. Tambi√©n puede instalarse desde PyPI o GitHub mediante el comando `pip install`.

---

## üåü Funciones clave de ADS

- **Conexi√≥n a fuentes de datos**: ADS provee conectores a muchas ubicaciones populares de datos.

- **Visualizaci√≥n de datos**: ADS tiene gr√°ficos inteligentes (*smart plotting*) con ajustes predeterminados seg√∫n el tipo de datos. Tambi√©n permite visualizar seg√∫n tipos de caracter√≠stica, correlaciones y relaciones entre variables.

- **Ingenier√≠a de caracter√≠sticas (Feature Type Engineering)**: ADS analiza los datos y da recomendaciones para transformar caracter√≠sticas y mejorar modelos.

- **Entrenamiento de modelos**: ADS permite entrenar modelos con AutoML de Oracle Labs o afinar hiperpar√°metros con ADSTuner. Tiene clases que empaquetan r√°pidamente modelos para su despliegue.

- **Evaluaci√≥n de modelos**: ADS incluye clases para evaluar desempe√±o de modelos con pocas l√≠neas de c√≥digo.

- **Interpretabilidad de modelos**: Entender y explicar lo que el modelo est√° aprendiendo es esencial para confiar en √©l y comunicarlo a otros.

- **Despliegue de modelos**: ADS lo facilita con clases para los tipos de modelos m√°s comunes, y tambi√©n para modelos gen√©ricos. Con unas pocas l√≠neas de c√≥digo, el modelo puede ponerse en producci√≥n.

---

## üîó Conexi√≥n a fuentes de datos

Los datos est√°n almacenados en muchos lugares, y necesitas poder acceder a ellos. A menudo, los datos son demasiado grandes para caber en tu sesi√≥n de notebook. Puedes usar ADS para limitar la transferencia de datos por la red.

**Almacenamiento local** es una ubicaci√≥n com√∫n para guardar tus datos. Este es el almacenamiento en bloques dentro de la sesi√≥n de notebook. ADS proporciona acceso f√°cil a eso.

Para conjuntos de datos m√°s grandes o para compartir conjuntos de datos, se utiliza com√∫nmente el **almacenamiento de objetos** (*Object Storage*). ADS utiliza el protocolo **APE Spec** para permitirte usar *pandas* y acceder al almacenamiento de objetos como si estuviera en tu disco local. Esto se realiza mediante el protocolo de OCI y *pandas* cuando el archivo est√° en almacenamiento de objetos.

Gran parte de nuestros datos est√°n en **bases de datos Oracle**. ADS proporciona una conexi√≥n f√°cil a estas bases de datos. Herramientas como **Oracle DB Secret Keeper** permiten almacenar credenciales de inicio de sesi√≥n en el archivo **DTP Wallet** dentro de un **OCI Vault**, de modo que no tengas que exponer esa informaci√≥n en tu notebook.

**ADB Secret Keeper** funciona con Autonomous Database. ADS proporciona integraci√≥n con proveedores de nube de terceros. Con ADS instalado, *pandas* puede conectarse a proveedores como S3, Google Cloud Storage, Azure Data Lake Storage, Azure Blob Service, Dropbox y muchos m√°s.

Para datos no relacionales, ADS proporciona acceso mediante la clase **Data Set Factory** para hacer conexiones simples con bases de datos NoSQL, ejecutar consultas y devolver resultados.

**OCI Big Data Service** es un servicio basado en Hadoop que utiliza **HDFS** como sistema de archivos.

ADS permite conectarse f√°cilmente a BDS sin necesidad de copiar los datos al almacenamiento local. Tambi√©n proporciona acceso a la web usando HTTP y HTTPS para leer archivos directamente hacia un dataframe.

---

## üìä Visualizaci√≥n de Datos

El an√°lisis exploratorio de datos es cr√≠tico para entender tus datos. Puede llevar tiempo crear y desechar clases, solo para tener que crearlas nuevamente la pr√≥xima vez que uses datos similares.

Las clases **Feature Type** proporcionan los mismos valores predeterminados para visualizar tus datos. Tambi√©n es muy f√°cil crear tipos de caracter√≠sticas personalizados para visualizar seg√∫n tus preferencias. Luego, puedes reutilizar estas visualizaciones en diferentes proyectos o en toda la organizaci√≥n.

Adem√°s, el sistema de tipos de caracter√≠sticas proporciona **estad√≠sticas resumidas**, **visualizaciones resumen** de cada caracter√≠stica y **mapas de calor de correlaciones**.

La **ingenier√≠a de caracter√≠sticas** puede ser un problema desafiante. Puede mejorar enormemente la calidad de tu modelo tomando caracter√≠sticas existentes y generando nuevas a partir de ellas‚Äîtransformando los datos que tienes en otros tipos de relaciones que el modelo pueda aprender.

ADS tiene funcionalidad integrada para apoyar esto. Hay una clase llamada **ADS Data Set** que envuelve un dataframe de *pandas*. Proporciona sugerencias de transformaci√≥n y puede hacerlo autom√°ticamente. Soporta codificaci√≥n categ√≥rica, valores nulos e imputaci√≥n. Puede ofrecer recomendaciones sobre qu√© cambios hacer a tus datos para crear mejores caracter√≠sticas.

---

## üß™ Entrenamiento de Modelos

Una vez que tengas tus datos preparados, es hora de crear un modelo. ADS puede automatizar completamente este proceso usando tecnolog√≠a **Auto Machine Learning**.

Puede probar muchos tipos diferentes de clases de modelo, ajustar hiperpar√°metros y proporcionar m√©tricas de desempe√±o para cada modelo.

**ADSTuner** realiza la **optimizaci√≥n de hiperpar√°metros**. Una vez entrenado el modelo, ADS tambi√©n puede empaquetar los archivos necesarios para crear un **artifact del modelo**, guardar ese artifact en el cat√°logo de modelos (*Model Catalog*), y luego puedes enviarlo a producci√≥n. Ya no necesitas luchar para poner tu modelo en producci√≥n.

---

## üìà Evaluaci√≥n de Modelos

Si tienes uno o varios modelos, puede que necesites entender el rendimiento y compararlos. **ADS Evaluator** permite realizar comparaciones entre modelos diferentes. Ofrece herramientas comunes, m√©tricas y gr√°ficos.

Entiende clasificaci√≥n **binaria**, **multinomial** y **regresi√≥n**, y genera m√©tricas y gr√°ficos apropiados para el tipo de problema. Ya no necesitas regenerar constantemente estos gr√°ficos o verificar si est√°s usando los correctos. ADS lo hace autom√°ticamente por vos.

---

## üîç Interpretabilidad del Modelo y Explicabilidad

Puedes desarrollar confianza en tu modelo si puedes explicar lo que ha aprendido y lo que est√° haciendo. Interpretar el comportamiento del modelo es clave para entender qu√© hace y qu√© mejoras pueden aplicarse en versiones futuras.

ADS ofrece herramientas para interpretar modelos que son **agn√≥sticas al tipo de modelo**, es decir, no dependen del tipo de modelo que se haya construido.

Proporciona explicaciones mediante el m√≥dulo de ADS que son interpretables, agn√≥sticas y ofrece herramientas para hacer **tests de escenarios hipot√©ticos** (*what-if*): cambiar valores de entrada y observar c√≥mo responde el modelo.

Tambi√©n ofrece **explicabilidad local**, es decir, permite entender por qu√© el modelo hizo una determinada predicci√≥n sobre una observaci√≥n espec√≠fica. Y ofrece **explicabilidad global**, para comprender qu√© ha aprendido el modelo y c√≥mo se comporta.

Lo hace mediante gr√°ficos como **Partial Dependence Plots** y **ALE Plots** (*Accumulated Local Effects*), para entender el comportamiento general. Usalos para verificar si el modelo est√° aprendiendo lo que deber√≠a y entender las relaciones entre los datos.

---

## üöÄ Despliegue de Modelos

Muchos cient√≠ficos de datos enfrentan un gran desaf√≠o al poner sus modelos en producci√≥n. Lo tienen corriendo en su notebook, pero ¬øc√≥mo hacerlo escalable y seguro?

ADS proporciona el **ADS Model Framework**, un conjunto de clases que permite desplegar modelos de distintos tipos. Con unos pocos comandos, pod√©s poner un modelo en producci√≥n.

ADS soporta colecciones de modelos como **Oracle Labs AutoML**, **PyTorch**, **scikit-learn**, **TensorFlow**, y muchos m√°s.

Tambi√©n tiene la capacidad de soportar **modelos gen√©ricos**. No importa el tipo de modelo que uses, puede ser desplegado con unos pocos comandos.

Una vez que tu modelo est√° en producci√≥n, necesitas entender qu√© est√° ocurriendo. Se integra con el servicio de **OCI Logging**, y crea **logs de predicci√≥n y acceso**. Esto te permite ver c√≥mo se accede al modelo y cu√°les fueron los resultados de predicci√≥n.

---

## üßæ Recapitulaci√≥n del M√≥dulo

En este m√≥dulo aprendiste que los objetivos del SDK son tres:

- Acceder e instalar la biblioteca ADS.
- Conectar a diferentes fuentes de datos y visualizar r√°pidamente el an√°lisis exploratorio.
- Recibir gu√≠a para la ingenier√≠a de caracter√≠sticas, entrenamiento y optimizaci√≥n de modelos.

Tambi√©n aprendiste c√≥mo evaluar modelos, entender su calidad, interpretarlos, y herramientas para desplegarlos en producci√≥n.

Muchas gracias.

---

## üß© Conceptos B√°sicos de Configuraci√≥n de Tenancy

Hola, soy Jon Stanesby. En esta lecci√≥n vamos a cubrir los conceptos b√°sicos de configuraci√≥n de **tenancy** para ciencia de datos.

Aunque es conocimiento com√∫n y probablemente solo un repaso para vos, repasemos r√°pidamente estos conceptos:

- **Compartments (Compartimientos)**: Son contenedores l√≥gicos para organizar recursos de OCI.
- **User Groups (Grupos de Usuarios)**: Simplemente un grupo de usuarios.
- **Dynamic Groups (Grupos Din√°micos)**: Grupos especiales de *principals* de recursos.
- **Policies (Pol√≠ticas)**: Se usan para otorgar acceso a grupos dentro de compartimientos.

Veamos c√≥mo estos componentes trabajan juntos para habilitar el acceso a los recursos de ciencia de datos:

1. Asign√°s usuarios a grupos de usuarios apropiados.
2. Creamos grupos din√°micos para recursos de ciencia de datos.
3. Finalmente, se crean pol√≠ticas que otorgan acceso a esos recursos dentro de un compartimiento.

---

## üß© Compartimientos

Empezando entonces con los **compartimientos**, estos te permiten organizar y controlar el acceso a tus recursos en la nube. Un compartimiento es un agrupamiento l√≥gico de recursos que solo pueden ser accedidos por ciertos grupos. Se les ha otorgado permiso por parte de un administrador.

Al configurar tu tenencia (*tenancy*), el primer paso es hacer un plan sobre c√≥mo vas a organizar tus recursos de ciencia de datos de ahora en adelante. Una vez que tengas el plan, puedes comenzar a crear uno o varios compartimientos. Mostraremos esto al final de la lecci√≥n. Por ahora, veamos el proceso r√°pido y sencillo de tres pasos para crear un compartimiento:

Desde la consola de Identidad, ve a **Identity** y selecciona **Compartments**. Haz clic en **Create Compartment**, ingresa un nombre y una descripci√≥n, y luego haz clic en **Create Compartment**.

---

## üë• Grupos de usuarios

Pasando ahora a los **grupos de usuarios**, son usuarios individuales que se agrupan en OCI y se les otorga acceso a los recursos de ciencia de datos dentro de los compartimientos.

Los administradores pueden realizar tres pasos simples para crear grupos de usuarios:

1. Crear los usuarios.
2. Crear los grupos.
3. Agregar usuarios a los grupos.

Al configurar grupos, primero decid√≠ c√≥mo acceder√°n los usuarios a los recursos dentro de los compartimientos.

---

## üîÑ Grupos din√°micos

Ahora, sobre un tipo especial de grupo llamado **grupos din√°micos** (*dynamic groups*), contienen recursos que coinciden con reglas que vos defin√≠s. Recursos como sesiones de notebook de ciencia de datos, ejecuciones de tareas (*job runs*), y despliegues de modelos pueden incluirse en un grupo din√°mico.

Estas reglas permiten que la membres√≠a del grupo cambie din√°micamente a medida que se crean o eliminan recursos que coincidan con esas reglas.

Estos recursos act√∫an como **principales** (*principal actors*). Pueden hacer llamadas a la API de servicios de acuerdo con las pol√≠ticas que hayas escrito para el grupo din√°mico. Veremos pol√≠ticas en breve.

Por ejemplo: usando el principal de recurso de una sesi√≥n de notebook en ciencia de datos, donde su grupo din√°mico tiene una pol√≠tica que habilita el acceso a almacenamiento de objetos, podr√≠as hacer una llamada a la API de Object Storage para leer datos desde un bucket.

Entonces, los recursos coinciden con reglas, y las reglas se aplican a grupos din√°micos. Una vez que le das a tu grupo din√°mico un nombre y una descripci√≥n, llen√°s las reglas de coincidencia (*matching rules*), donde el OCID del compartimiento es reemplazado por el identificador del compartimiento que creaste para ciencia de datos.

En este ejemplo, con estas reglas, el grupo din√°mico estar√° compuesto por todos los recursos de esos tres tipos que existan en el compartimiento. Lo que estos recursos pueden hacer, como se mencion√≥ antes, depender√° de las **pol√≠ticas**.

---

## üõ°Ô∏è Pol√≠ticas

Las **pol√≠ticas** definen qu√© principales, como usuarios y recursos, tienen acceso en OCI. El acceso se otorga a nivel de grupo y de compartimiento, lo que significa que pod√©s escribir una pol√≠tica que d√© a un grupo un tipo espec√≠fico de acceso dentro de un compartimiento espec√≠fico.

Las pol√≠ticas tienen una sintaxis b√°sica:

> `allow group nombre-del-grupo to do acci√≥n on tipo-de-recurso in compartment nombre-del-compartimiento`

Veamos m√°s de cerca cada variable de la sintaxis:

- **Group name**: nombre del grupo de usuarios o grupo din√°mico.
- **Verb (Verbo)**: define el nivel de acceso. Veremos los tipos de verbo a continuaci√≥n.
- **Resource type**: especifica el tipo de recurso o familia de recursos.
- **Compartment name**: nombre del compartimiento.

---

## ‚úÖ Tipos de verbos (acciones permitidas)

Los verbos definen el nivel de acceso permitido al recurso o familia de recursos. De menor a mayor permisividad:

- `inspect`: permite listar recursos sin acceder a metadatos definidos por el usuario.
- `read`: incluye inspect + acceso a metadatos y al recurso mismo.
- `use`: incluye read + posibilidad de trabajar con el recurso (por ejemplo, actualizar). No suele incluir crear o eliminar.
- `manage`: incluye todos los permisos, incluyendo creaci√≥n y eliminaci√≥n.

---

## üß¨ Tipos de recursos

El tipo de recurso en la pol√≠tica define para qu√© recurso est√°s escribiendo la pol√≠tica. Por ejemplo, ‚Äúdata science‚Äù incluye modelos y tareas de ciencia de datos.

Para facilitar la escritura de pol√≠ticas para recursos relacionados, hay tipos **agregados** que abarcan familias. El tipo agregado para ciencia de datos es `data-science-family`.

üî∏ Importante: estos son ejemplos **cr√≠ticos** de pol√≠ticas requeridas, no simplemente ilustraciones.

1. Pol√≠tica para permitir que cient√≠ficos de datos gestionen todos los recursos de ciencia de datos en un compartimiento espec√≠fico:

```plaintext
allow group tu-grupo-de-usuarios to manage data-science-family in tu-compartimiento
```

2. Pol√≠tica para permitir que recursos de ciencia de datos (como notebook sessions), dentro de un grupo din√°mico que hayas creado, gestionen todos los recursos de ciencia de datos:

```plaintext
allow dynamic-group tu-grupo-dinamico to manage data-science-family in tu-compartimiento
```

---

## üìä Pol√≠ticas para m√©tricas y logs

Las siguientes pol√≠ticas permiten acceso a m√©tricas y registros (*logging*):

- Permitir que el grupo de usuarios lea m√©tricas:

```plaintext
allow group tu-grupo to read metrics in compartment tu-compartimiento
```

- Permitir que el grupo din√°mico use contenido de logs:

```plaintext
allow dynamic-group tu-grupo to use log-content in compartment tu-compartimiento
```

- Permitir que el grupo de usuarios gestione grupos de logs:

```plaintext
allow group tu-grupo to manage log-groups in compartment tu-compartimiento
```

- Permitir que el grupo de usuarios use contenido de logs:

```plaintext
allow group tu-grupo to use log-content in compartment tu-compartimiento
```

---

## üåê Pol√≠ticas para red personalizada

Si plane√°s usar networking personalizado (tema del pr√≥ximo m√≥dulo), necesitar√°s estas pol√≠ticas:

- Para el servicio de data science:

```plaintext
allow service data-science to use virtual-network-family in compartment tu-compartimiento
```

- Para el grupo de usuarios:

```plaintext
allow group tu-grupo to use virtual-network-family in compartment tu-compartimiento
```

- Para el grupo din√°mico:

```plaintext
allow dynamic-group tu-grupo to use virtual-network-family in compartment tu-compartimiento
```

---

## üõ†Ô∏è Creaci√≥n de recursos en la consola de Identidad

- **Crear un compartimiento**: ir a Compartments ‚Üí Create Compartment ‚Üí agregar nombre, descripci√≥n y etiquetas (opcional). Esperar unos momentos hasta su creaci√≥n. Anotar el OCID para uso posterior.

- **Crear un usuario**: ir a Users ‚Üí Create User ‚Üí a√±adir nombre de usuario, descripci√≥n y correo electr√≥nico. Repetir para cada usuario que desees agregar.

- **Crear un grupo de usuarios**: ir a Groups ‚Üí Create Group ‚Üí a√±adir nombre y descripci√≥n ‚Üí Create. Luego, hacer clic en Add User to Group, seleccionar el usuario y confirmar.

- **Crear grupo din√°mico**: ir a Dynamic Groups ‚Üí Create Dynamic Group ‚Üí a√±adir nombre y descripci√≥n. Luego ingresar las reglas de coincidencia:

   1. Para sesiones de notebook.
   2. Para despliegues de modelos.
   3. Para ejecuciones de tareas.

   Sustituir el ID del compartimiento por el que creaste antes. Hacer clic en Create para finalizar.

---

## üîê Paso final: crear pol√≠ticas para habilitar acceso

Ahora, para el paso final, necesitamos permitir que nuestros recursos y usuarios accedan a ciencia de datos en nuestro compartimiento. Para esto vamos a crear una **pol√≠tica**.

En esta pol√≠tica, voy a cubrir las pol√≠ticas requeridas para ciencia de datos. Le doy un nombre y descripci√≥n relevante. En el **Policy Builder**, cambio al editor manual para poder pegar mis declaraciones de pol√≠tica. Una vez agregadas, hago clic en **Create** para crear la pol√≠tica.

Hab√≠a algunas pol√≠ticas adicionales requeridas, espec√≠ficamente para que usuarios y grupos din√°micos accedan a m√©tricas. As√≠ que edito esta pol√≠tica y agrego las siguientes declaraciones:

- Que mi grupo de usuarios pueda leer m√©tricas en el compartimiento.
- Que mi grupo din√°mico acceda a contenido de logs.
- Que mi grupo de usuarios acceda a grupos de logs.
- Finalmente, que mi grupo de usuarios acceda a contenido de logs.

Una vez agregadas estas declaraciones, guardo mis cambios.

Ya que cubrimos las pol√≠ticas requeridas, mostramos algunas pol√≠ticas **√∫tiles** que tambi√©n conviene crear, especialmente cuando queremos que los recursos o usuarios de un servicio de ciencia de datos accedan a otros servicios de OCI.

En este caso, estoy agregando **pol√≠ticas √∫tiles** para ciencia de datos. Nuevamente, les doy un nombre y una descripci√≥n, y cambio al editor manual para agregarlas. Estas pol√≠ticas son espec√≠ficas para **Object Storage**, as√≠ que quiero permitir que **mi grupo din√°mico y mi grupo de usuarios** gestionen la familia de objetos (*object-family*) en mi compartimiento. Puedo repetir este proceso para todas las pol√≠ticas que quiera agregar.

---

## üß≠ Resumen de conceptos de configuraci√≥n de tenencia

- Compartimientos
- Grupos de usuarios
- Grupos din√°micos
- Pol√≠ticas

Repasamos reglas de coincidencia para agrupar recursos en grupos din√°micos, la sintaxis de las pol√≠ticas y sus variantes. Discutimos las pol√≠ticas requeridas para ciencia de datos, las relacionadas, y algunas opcionales que pueden resultar √∫tiles.

Gracias.

---

## ‚öôÔ∏è Configurar la tenencia con OCI Resource Manager

Hola, soy John Stanesby. En esta lecci√≥n vamos a mostrar c√≥mo configurar una tenencia con **OCI Resource Manager**.

En vez de configurar tu tenencia manualmente, pod√©s usar la **plantilla del servicio de ciencia de datos** que viene preconfigurada en Oracle Resource Manager. Esta plantilla crea autom√°ticamente los grupos de usuarios, grupos din√°micos y pol√≠ticas requeridas para un caso b√°sico.

La plantilla crea:

- Un grupo de usuarios (nombre definido por vos).
- Un grupo din√°mico (nombre definido por vos).
- Reglas de coincidencia para: `datasciencenotebooksession`, `datasciencemodeldeployment`, `datasciencejobrun`.

Tambi√©n crea una pol√≠tica con las siguientes declaraciones:

- Permitir que el grupo de usuarios gestione `data-science-family` en el compartimiento.
- Permitir que el grupo din√°mico gestione `data-science-family` de recursos en el compartimiento.
- Permitir que el grupo de usuarios lea m√©tricas en el compartimiento.
- Permitir que el grupo din√°mico use contenido de logs en el compartimiento.

Haremos una demo al final de la lecci√≥n.

---

## üîÅ Proceso general para usar el ORM Stack

1. Crear el **Stack**.
2. Seleccionar tu plantilla.
3. Seleccionar el compartimiento.
4. Ejecutar el stack.
5. Agregar usuarios al grupo creado.

Record√°: las plantillas solo est√°n disponibles en la consola. Y pod√©s editar tu stack en cualquier momento.

---

## üß± Opciones alternativas

Adem√°s de usar la plantilla de muestra, tambi√©n pod√©s usar tu propio **script Terraform**, ubicado en este repo p√∫blico de GitHub.

Ahora voy a mostrar la configuraci√≥n de una tenencia con OCI Resource Manager:

- Navegar a Resource Manager ‚Üí Stacks ‚Üí Create Stack.
- Seleccionar **Template** como origen.
- Ir a **Service** y elegir **Data Science** ‚Üí Select Template.
- Elegir el compartimiento deseado ‚Üí Next.
- Completar variables adicionales si quer√©s ‚Üí Next.
- Ejecutar `apply` sobre el stack ‚Üí Click Create.
- Esperar que corra el job.
- Una vez creado, solo necesit√°s agregar tus usuarios al grupo generado.

Tambi√©n pod√©s acceder al script Terraform en el repo p√∫blico de GitHub.

---

## üì° Networking para Ciencia de Datos

Hola, soy Jon Stanesby. En esta lecci√≥n veremos **networking en ciencia de datos**. Vamos a presentar algunos componentes √∫tiles de red en la nube, con una introducci√≥n de alto nivel para entender c√≥mo se relacionan con ciencia de datos. Este curso no entra en profundidad sobre networking.

### üîß Componentes clave:

- **VCN (Virtual Cloud Network)**  
- **Subnets**
- **VNICs (Virtual Network Interface Cards)**
- **DRG (Dynamic Routing Gateway)**
- **NAT Gateway (Network Address Translation)**
- **Service Gateway**

---

## üîå C√≥mo trabajan juntos

- El **VCN** es una red privada virtual que configur√°s en los data centers de Oracle.
- Las **subnets** son subdivisiones dentro de un VCN. Contienen **VNICs**, que se adjuntan a instancias.
- Todas las VNICs en una subnet comparten las mismas pol√≠ticas de red: tabla de rutas, listas de seguridad y opciones DHCP.

---

### üö™ Enrutadores virtuales opcionales

1. **DRG**: Provee ruta de tr√°fico privado entre el VCN y tu red on-premise.  
   Se puede usar para establecer conexi√≥n mediante VPN sitio a sitio o **FastConnect**.

   Tambi√©n conecta tu VCN con otro VCN en otra regi√≥n.

   Da acceso a internet para recursos sin IP p√∫blica, sin exponerlos a conexiones entrantes.

2. **Service Gateway**: Permite tr√°fico privado entre tu VCN y servicios de Oracle.  
   Ejemplo: sistemas de bases de datos en subnet privada pueden respaldar datos en Object Storage sin IP p√∫blica ni acceso a internet.

---

## üñ•Ô∏è Workloads en ciencia de datos

Pod√©s crear varios tipos de recursos que ejecutan c√≥digo para distintos usos:

- **Notebook sessions**
- **Jobs y job runs**
- **Model deployments**

En esta lecci√≥n los llamaremos **workloads**.

Muchas veces vas a querer acceder a recursos externos desde tu workload: archivos de c√≥digo, datos, bibliotecas, secretos y logs.

Tambi√©n podr√≠as querer ejecutar otros workloads en Data Science o en otra plataforma como Data Flow.

Estos recursos externos pueden estar en internet p√∫blico o en una red privada.

---

## üåê Patrones de Networking

Para acceder a estos recursos, necesit√°s conectividad entre tu workload y la ubicaci√≥n de red donde est√©n.

Hay **dos patrones de red** que pod√©s usar:

1. **Default networking**:  
   El workload se conecta mediante una VNIC secundaria a una subnet gestionada por el servicio.

   Esta subnet permite salida a internet por un **NAT Gateway** y acceso a servicios de OCI v√≠a **Service Gateway**.

   Si solo necesit√°s acceso a internet y/o servicios OCI, esta es la forma m√°s r√°pida y sencilla de comenzar, ya que no requiere crear recursos de red propios ni escribir pol√≠ticas de permisos.

---

## üåê Configuraci√≥n personalizada de red

Cuando seleccion√°s **custom networking** (red personalizada) al crear un recurso de ciencia de datos, vas a especificar una **subnet preexistente** que pertenezca a tu tenencia y que quer√©s usar para los workloads (cargas de trabajo) de ciencia de datos. Cuando se crea el workload, el servicio de ciencia de datos se conectar√° a tu subnet seleccionada mediante una conexi√≥n secundaria con VNIC (interfaz de red virtual).

Esta configuraci√≥n de ‚Äútrae tu propia red‚Äù (*bring-your-own-network*) mediante red personalizada te permitir√° acceder a los recursos y activos definidos por tu subnet.

Si necesit√°s acceso a activos externos dentro de una **red privada**, como archivos de c√≥digo en un servidor empresarial de Git o datos en una base de datos on-prem, vas a necesitar usar **custom networking** para asegurar la conectividad de tus workloads. Por favor, trabaj√° con el administrador de red de tu tenencia para configurar tu subnet VCN para ciencia de datos.

Como se discuti√≥ en la lecci√≥n sobre configuraci√≥n de tenencia, necesitar√°s **pol√≠ticas adicionales** para usar red personalizada en ciencia de datos.

---

## üß∞ Asistente de configuraci√≥n r√°pida de red (VCN Wizard)

Voy a mostrarte una forma r√°pida de configurar red para ciencia de datos. Para hacerlo, vamos a usar el **VCN Wizard**.

- Naveg√° a **Networking > Virtual Cloud Networks**.
- Hac√© clic en **Start VCN wizard**, y luego eleg√≠ **Create VCN with Internet Connectivity**.
- Al iniciar el asistente, solo necesit√°s darle un **nombre** a tu VCN.

Al desplazarte hacia abajo, ver√°s varias opciones que suelen usar los usuarios avanzados. Si quer√©s continuar con la red por defecto, simplemente hac√© clic en **Next** y luego en **Create**.

Esper√° un momento mientras se crean varios recursos dentro de tu compartimiento. Una vez finalizado, hac√© clic en **View Virtual Cloud Network**.

Ahora podemos ver que nuestra red ha sido creada. Si vuelvo a la pantalla de **Virtual Cloud Network**, puedo ver que mi ejemplo *DS VCN* ya fue creado.

üìù *Nota: no necesit√°s realizar este paso si configuraste tu tenencia usando OCI Resource Manager, ya que el VCN se crea autom√°ticamente.*

---

## üßµ Conectividad: red por defecto vs red personalizada

En esta lecci√≥n vimos:

- Componentes y definiciones de red en la nube.
- C√≥mo se combinan esos componentes.
- Las dos opciones de conectividad: **default networking** o **custom networking**.

---

## üîë Autenticaci√≥n en APIs de OCI

Hola, soy Jon Stanesby. En esta lecci√≥n veremos c√≥mo autenticarse en las **APIs de OCI**.

Los recursos de ciencia de datos (como notebook sessions, jobs y model deployments) te permiten ejecutar c√≥digo personalizado.

Como parte de tu c√≥digo, podr√≠as querer interactuar con otros servicios de OCI mediante las APIs REST. Esto permitir√≠a, por ejemplo, leer o escribir datos en Object Storage desde un job, o crear/ejecutar aplicaciones de Data Flow desde una notebook session.

Para interactuar con las APIs de OCI, necesit√°s operar como un **usuario autenticado**.

En ciencia de datos, los m√©todos m√°s comunes para interactuar con las APIs de OCI son:

- **ADS SDK**
- **OCI Python SDK**
- **OCI Command Line Interface (CLI)**

Esta lecci√≥n explica las opciones de autenticaci√≥n para cada uno de estos interfaces.

üìå *Importante: esta lecci√≥n solo trata sobre autenticaci√≥n (verificaci√≥n de identidad reconocida por OCI), no sobre autorizaci√≥n (nivel de acceso), ya cubierto en la lecci√≥n 2 del m√≥dulo de configuraci√≥n de tenencia.*

---

## üë• Principios de recurso (Resource Principals)

Un **resource principal** es una funcionalidad de IAM (Identity and Access Management) que permite que los recursos sean actores principales autorizados para realizar acciones sobre otros servicios.

Cada recurso tiene su propia identidad, y se autentica usando certificados que se le asignan autom√°ticamente. Estos certificados son creados, asignados y rotados sin que vos tengas que guardar credenciales en tu sesi√≥n de notebook o job.

El servicio de ciencia de datos permite autenticarse usando el **resource principal** de la notebook session o de la ejecuci√≥n del job para acceder a otros recursos de OCI. Este m√©todo es m√°s seguro que usar configuraci√≥n de OCI y claves API.

Adem√°s, es m√°s pr√°ctico para jobs que no tienen una interfaz interactiva como la notebook para crear y mover archivos de configuraci√≥n.

üîê *Si no us√°s expl√≠citamente resource principals al invocar un SDK o CLI, se usar√° el enfoque tradicional de archivo de configuraci√≥n + clave API.*

---

## ‚è≥ Token de resource principal

- Se **almacena en cach√© por 15 minutos**.
- Si cambi√°s la pol√≠tica o el grupo din√°mico, deber√°s **esperar 15 minutos** para que el cambio tenga efecto.
- El c√≥digo para establecer resource principal como mecanismo de autenticaci√≥n var√≠a levemente seg√∫n el interfaz usado.

üìå Puede ser √∫til **pausar la lecci√≥n y anotar los ejemplos de c√≥digo**.

---

## üë§ Autenticarse como usuario IAM personal

Pod√©s operar como tu propio usuario IAM creando:

- Un **archivo de configuraci√≥n de OCI**.
- Una **clave API (.pem)**

Este es el m√©todo por defecto con ADS, OCI SDK de Python o CLI.

Para autenticarte con este enfoque, deb√©s:

1. Subir tu archivo de configuraci√≥n de OCI a la carpeta `oci/` dentro de la notebook session.
2. Subir o crear los archivos `.pem` necesarios para el perfil definido.

üìì *En vez de subir archivos existentes, pod√©s usar el notebook `api_key` para generarlos.*

- Para lanzarlo, hac√© clic en **Notebook Examples** desde la pesta√±a de **JupyterLab Launcher**.

---

## üßæ Recapitulaci√≥n

En esta lecci√≥n cubrimos:

- La importancia de la autenticaci√≥n en ciencia de datos.
- La necesidad de autenticar diferentes interfaces.
- Definici√≥n de resource principals.
- C√≥mo se combinan con el servicio de ciencia de datos.
- C√≥mo trabajar con resource principals en distintos entornos.
- Uso de archivos de configuraci√≥n de OCI.

</br>
</br>
</br>




---
<h1>## üß† Unidad 3: Dise√±o y configuraci√≥n del espacio de trabajo</h1>

### üéì M√≥dulo 2: Workspace Design and Setup  
Instructor: *John Stanseby*
---

<h2>Projects</h2>
## üìå Tema central: El proyecto como componente principal

Un **proyecto de ciencia de datos** en OCI es un espacio colaborativo donde los equipos organizan su trabajo en torno a un caso de uso o pregunta de negocio.  
Todos los recursos de ciencia de datos (como notebooks y modelos) se crean **dentro de un proyecto**.

---

## üõ†Ô∏è Creaci√≥n de proyectos

### 1. Desde la **Consola de OCI**

Pasos:
- Iniciar sesi√≥n con pol√≠ticas adecuadas
- Ir a **Analytics & AI ‚Üí Machine Learning ‚Üí Data Science**
- Clic en **Create Project**
- Seleccionar el **compartimento**

Opciones recomendadas:
- **Nombre √∫nico** (si no se define, se genera autom√°ticamente con timestamp)
- **Descripci√≥n** (√∫til para otros usuarios)
- **Tags decorativos**: seleccionar namespace, clave y valor  
  ‚Üí Para m√°s de un tag: clic en *Add Additional Tags*

Al finalizar:
- Clic en **View Detail page** y luego en **Create**
- Se habilita la creaci√≥n de notebooks y modelos asociados al proyecto

---

### 2. Desde el **SDK de ADS (Python)**

Usar el objeto `ProjectCatalog` y el m√©todo `create_project`, especificando el `compartment_id`.  
Ejemplo: usar la variable de entorno del notebook para crear el proyecto en el mismo compartimento.

---

## üßæ Gesti√≥n de proyectos

### üîç Visualizaci√≥n
- Ir a la **Project List page**
- Clic en un proyecto para ver detalles:  
  - Nombre, descripci√≥n  
  - OCID  
  - Fecha y autor de creaci√≥n  
  - Tags

### ‚úèÔ∏è Edici√≥n
- Solo se pueden editar:  
  - Nombre  
  - Descripci√≥n  
  - Tags (desde el men√∫ de detalles)

### üóëÔ∏è Eliminaci√≥n
- Requisito: el proyecto debe estar **vac√≠o**
- Eliminar notebooks, modelos y recursos asociados primero
- Confirmar escribiendo el **nombre exacto** del proyecto
- Estado pasa a **"deleting"**, luego a **"deleted"**
- Se puede filtrar por estado (ej. ocultar los eliminados)

---

## üß© Consideraciones clave

| Elemento | Recomendaci√≥n |
|---------|----------------|
| Nombre | √önico, descriptivo |
| Descripci√≥n | Breve, clara, √∫til para terceros |
| Tags | Decorativos, funcionales, agrupables |
| Compartimento | Bien definido, con pol√≠ticas IAM adecuadas |
| Eliminaci√≥n | Solo si est√° vac√≠o, con confirmaci√≥n exacta |

---

<h2>Notebook Sessions en OCI</h2>

### üß† ¬øQu√© son?
- Interfaces JupyterLab gestionadas por OCI para construir y entrenar modelos ML.
- Infraestructura totalmente administrada: no requiere interacci√≥n directa con APIs de c√≥mputo o almacenamiento.

### ‚öôÔ∏è Caracter√≠sticas clave
- Soporte para CPU y GPU (AMD, Intel, Nvidia).
- Almacenamiento persistente para datos, notebooks y entornos.
- Actualizaciones y parches autom√°ticos.
- Escalables: pod√©s cambiar forma de c√≥mputo y tama√±o de almacenamiento.

### üõ†Ô∏è Creaci√≥n desde la consola
1. Crear un proyecto.
2. Ir a la p√°gina de detalles del proyecto ‚Üí *Create notebook session*.
3. Seleccionar compartimento (`HotelConciliacionML`, por ejemplo).
4. Elegir forma de c√≥mputo (ej. Standard 2.2).
5. Definir tama√±o de almacenamiento (m√≠nimo 50 GB).
6. Configurar red (default o custom con VCN y subnet).
7. Agregar tags decorativos.
8. Click en *Create* ‚Üí tarda unos minutos en activarse.

### üìã Gesti√≥n de sesiones
- **Visualizaci√≥n**: desde la lista de sesiones o detalles del proyecto.
- **Edici√≥n**: solo el nombre es editable cuando est√° activa.
- **Eliminaci√≥n**: requiere confirmar el nombre; se destruye instancia y volumen.
- **Activaci√≥n/Desactivaci√≥n**:
  - Activar: permite escalar recursos.
  - Desactivar: apaga instancia pero conserva el volumen (excepto boot volume).
  - Reactivar: se crea nueva instancia y se adjunta el volumen anterior.

### üìä M√©tricas disponibles
- Uso de CPU y memoria.
- Tr√°fico de red (bytes in/out).

---
<h2>JupyterLab</h2>

## üß™ Unidad: C√≥mo trabajar con JupyterLab en OCI

### üß† ¬øQu√© es JupyterLab?
- Interfaz web de pr√≥xima generaci√≥n para notebooks.
- Utilizada en las *notebook sessions* de OCI por su familiaridad con data scientists.
- Permite integrar notebooks, editores de texto, terminales y componentes personalizados.

---

## üì¶ Funcionalidades principales

### üîπ Soporte de formatos
- Compatible con: `.ipynb`, `.txt`, `.csv`, `.json`, `.md`, `.pdf`, im√°genes, y visualizaciones Vega/Vega Lite.

### üîπ Diferencias con JupyterLab open source
Aunque la estructura es similar, en OCI se agregan:
- **Launcher personalizado** con acceso directo a notebooks, terminales, editores y ejemplos.
- **Environment Explorer**: GUI para gestionar entornos Conda.
- **Extensi√≥n GitHub**: para control de versiones dentro del notebook.

---

## üß≠ Componentes de la interfaz

### üîù Barra superior (Chrome bar)
- Logo de Oracle: vuelve a la consola principal.
- Nombre de la sesi√≥n: lleva al detalle del notebook.
- Tiempo restante: por defecto 1 hora de inactividad ‚Üí se puede extender hasta 24 horas.
- Icono de ayuda: acceso a documentaci√≥n.
- Cierre de sesi√≥n.

### üìÅ Barra lateral izquierda
- **Explorador de archivos**: navegar, abrir, crear y borrar carpetas.
- **Terminal y kernels activos**: detener procesos.
- **Extensi√≥n Git**: gesti√≥n de versiones (cubierta en otra lecci√≥n).
- **Comandos**: acceso a todos los comandos de JupyterLab.
- **Inspector de propiedades**: activo en notebooks.
- **Lista de pesta√±as abiertas**
- **Tabla de contenidos**: generada desde Markdown.
- **Gestor de extensiones instaladas**

üëâ Para ocultar la barra lateral: hacer doble clic en cualquier √≠cono.

---

## üß© √Årea de trabajo principal
- Paneles de pesta√±as redimensionables.
- Actividad actual marcada con borde azul.
- **Code Consoles**: espacio temporal para ejecutar c√≥digo interactivo.
- **Kernel-backed documents**: permiten ejecutar c√≥digo desde cualquier archivo de texto.
- **Vista m√∫ltiple**: edici√≥n en vivo desde distintos editores o visores.
- **Gesti√≥n de kernels**: desde el men√∫ *Kernel*, se accede a acciones como reiniciar, detener o cambiar kernel.


---

## üß™ Unidad: C√≥mo trabajar con JupyterLab ‚Äì Parte 2

### üéØ Objetivo
Explorar el uso del **Launcher**, la creaci√≥n de notebooks, el manejo de celdas, kernels, extensiones y herramientas visuales dentro de JupyterLab en OCI.

---

## üöÄ El Launcher

- Acceso r√°pido a:
  - Notebooks
  - Consolas
  - Editor de texto
  - Terminales
  - **Environment Explorer**
  - **Notebook Examples**

- Parte superior: enlaces √∫tiles para empezar, documentaci√≥n y ejemplos.
- Lado izquierdo: extensiones destacadas:
  - **Environment Explorer**: descubre y gestiona entornos Conda.
  - **Notebook Examples**: accede a tutoriales y casos de uso.

- Lado derecho: crear archivos nuevos (notebooks, Markdown, texto) o sesiones (terminal, consola).

---

## üìì Creaci√≥n y uso de notebooks

### üîπ Crear notebook con kernel Python3
- Clic en el kernel ‚Üí *Create Notebook*
- Se genera un archivo con tips √∫tiles:
  - Verificaci√≥n de conectividad
  - Imports t√≠picos de ADS
  - Variables de entorno

### üîπ Acciones b√°sicas
- **Renombrar**: clic derecho ‚Üí *Rename*
- **Ejecutar c√≥digo**:
  - Clic en tri√°ngulo
  - Men√∫ *Run ‚Üí Selected Cell*
  - Atajo: `Shift + Enter`
- **Indicadores**:
  - ‚≠ê Estrella: celda en ejecuci√≥n
  - N√∫mero: orden de ejecuci√≥n

### üîπ Tipos de celda
- Cambiar entre `Code`, `Markdown`, etc.
- Ejemplo Markdown:  
  ```markdown
  # T√≠tulo
  ```

- Reordenar celdas: arrastrar desde el borde izquierdo

---

## ‚úèÔ∏è Men√∫ Edit

- **Merge cells**: combinar celdas seleccionadas
- **Split cells**: dividir celdas
- **Cambiar kernel**: clic en nombre del kernel ‚Üí seleccionar otro

---

## üìä Ejemplo avanzado: notebook de clasificaci√≥n binaria

- Desde el launcher ‚Üí *Notebook Examples* ‚Üí *Binary Classification Attrition* ‚Üí *Load Example*
- Ejecutar todas las celdas
- Para detener: *Kernel ‚Üí Interrupt Kernel*

---

## üîç Herramientas visuales

### üîπ Variable Inspector
- Ver variables activas
- Posicionar junto al notebook: arrastrar pesta√±a hasta que se marque en azul

### üîπ Men√∫ View
- Mostrar n√∫meros de l√≠nea
- Colapsar/expandir celdas y salidas

---

## üß∞ Explorador de archivos

- Crear archivos: clic derecho
- Subir archivos: arrastrar y soltar
- Visualizaci√≥n depende del tipo:
  - `.csv`: muestra columnas y filas

---

## üñ•Ô∏è Terminal

- Comandos Linux est√°ndar (`ls`, etc.)
- Herramientas disponibles:
  - `ODSC conda CLI`
  - `Git CLI`
  - `OCI CLI`

---

## üìö Men√∫ Help

- Acceso a:
  - Documentaci√≥n
  - FAQs
  - Material de referencia

---

## üß© Aplicaci√≥n en tus flujos

Pod√©s usar JupyterLab para:
- Documentar pruebas con Markdown y tabla de contenidos
- Versionar c√≥digo con Git desde la sesi√≥n
- Ejecutar modelos y visualizar m√©tricas
- Usar terminal para automatizaciones con OCI CLI
- Explorar entornos Conda para pruebas familiares

---
</br>
</br>

# <h1>Conda Environments en OCI Data Science</h1>

---

## üß™ ¬øQu√© es un Conda Environment?

Un **Conda Environment** es un contenedor de software que incluye:

- Int√©rprete (ej. Python)
- M√≥dulos y librer√≠as espec√≠ficas (ej. `scikit-learn`, `pandas`, `tensorflow`)
- Configuraciones personalizadas

Permite trabajar de forma **aislada, reproducible y compartible**.

---

## üéØ Beneficios clave

- **Instalaci√≥n selectiva**: solo los paquetes que necesit√°s
- **Aislamiento**: distintos entornos para distintos modelos (ej. visi√≥n vs regresi√≥n)
- **Cambio r√°pido**: pod√©s alternar entre entornos sin conflictos
- **Compartibilidad**: se pueden compartir entre notebook sessions o con tu equipo
- **Reproducibilidad**: pod√©s volver a la versi√≥n exacta de librer√≠as usadas en un modelo

---

## üß≠ Tipos de Conda Environments en OCI

| Tipo                        | ¬øQui√©n lo gestiona? | ¬øD√≥nde se usa?                          |
|-----------------------------|---------------------|------------------------------------------|
| **Data Science Conda**      | Oracle               | Preinstalados, listos para usar          |
| **Published Conda**         | Vos (el usuario)     | Guardados en Object Storage, compartibles |
| **Installed Conda**         | Notebook session     | Activos en tu sesi√≥n, persistentes en block volume |

---

## üñ•Ô∏è Environment Explorer

Una interfaz gr√°fica dentro de JupyterLab que te permite:

- Ver entornos en **vista de tarjetas** o **lista**
- Buscar por palabra clave
- Filtrar por GPU, estado (activo/deprecado), tipo
- Instalar, clonar o eliminar entornos
- Ver detalles t√©cnicos y librer√≠as incluidas

---

</br>
</br>

<h1> Conda </h1>
---
# üß† Teor√≠a de los Entornos Conda en OCI Data Science

## 1. ¬øQu√© es un entorno Conda en OCI?

Un entorno Conda en el servicio de Data Science de Oracle Cloud Infrastructure (OCI) es un paquete preconfigurado de herramientas, librer√≠as y configuraciones que permite desarrollar, entrenar y desplegar modelos de machine learning de forma eficiente. Estos entornos est√°n basados en software open source y pueden personalizarse seg√∫n las necesidades del proyecto.

### ‚úÖ Ventajas:
- Ahorra tiempo en la configuraci√≥n inicial.
- Incluye librer√≠as optimizadas para tareas espec√≠ficas.
- Compatible con JupyterLab y el ecosistema de OCI.
- Permite trabajar con CPU o GPU seg√∫n el entorno elegido.

---

## 2. Acceso y uso

Los entornos Conda se acceden desde la pesta√±a **Launcher** de JupyterLab, mediante el bot√≥n **Environment Explorer**. Todos los entornos incluyen:

- **OCI Python SDK**: para interactuar con servicios de OCI.
- **ADS SDK (oracle-ads)**: para flujos de machine learning, AutoML, ingesti√≥n de datos y explicabilidad de modelos.

---

## 3. Tipos de entornos Conda

### üîß Seg√∫n aplicaci√≥n:
- **ONNX**: para portabilidad de modelos entre frameworks.
- **PyPGX**: para grafos y an√°lisis de redes.
- **PySpark**: para procesamiento distribuido con Apache Spark.

### üéØ Seg√∫n caso de uso:
- **Computer Vision**: procesamiento de im√°genes y video.
- **Data Exploration & Manipulation**: an√°lisis exploratorio y visualizaci√≥n.
- **Financial Services**: flujos espec√≠ficos para servicios financieros.
- **Natural Language Processing (NLP)**: procesamiento de texto y lenguaje.

---

## 4. Familias de entornos Conda

Los entornos se agrupan por:

- **Versi√≥n de Python** (ej. 3.6, 3.7).
- **Arquitectura** (CPU o GPU).
- **Versi√≥n del entorno** (v1, v2, etc.).

Ejemplo:  
`Natural Language Processing for CPU - Python 3.7 - v2`  
‚Üí Misma arquitectura y versi√≥n de Python que v1, pero con librer√≠as actualizadas.

---

## 5. Convenci√≥n de nombres

### üß© Basados en aplicaci√≥n:
`TensorFlow 2.7 for CPU - Python 3.7 - v1`

### üß™ Basados en tarea:
`Data Exploration and Manipulation for CPU - Python 3.7`

---

## 6. Entornos populares y sus librer√≠as clave

### üì∑ Computer Vision
- Tareas: detecci√≥n de objetos, reconocimiento facial, seguimiento ocular.
- Librer√≠as: `scikit-image`, `Pillow`, `PyTorch`, `OpenCV`.

### üìä Data Exploration & Manipulation
- Tareas: ingesti√≥n, visualizaci√≥n, an√°lisis exploratorio.
- Librer√≠as: `ADS`, `Kafka`, `pandas`, `pandaparallel`, `matplotlib`, `seaborn`, `plotly`, `bokeh`.

### ü§ñ General Machine Learning
- Tareas: ML supervisado, AutoML, explicabilidad.
- Librer√≠as: `xgboost`, `lightgbm`, `keras`, `TensorFlow`, `Oracle AutoML`, `Oracle MLX`.

### üìù Natural Language Processing
- Tareas: extracci√≥n de texto, frases clave, POS tagging.
- Librer√≠as: `nltk`, `keybert`, `pytorch-lightning`, `simpletransformers`.

### üîÑ ONNX
- Tareas: conversi√≥n y ejecuci√≥n de modelos en formato portable.
- Librer√≠as: `ONNX`, `ONNX Runtime`.

### üóÉÔ∏è Oracle Database
- Tareas: ETL, transformaciones, consultas SQL.
- Librer√≠as: `ipython-sql`, `SQLAlchemy`, `ADS Connector`.

### ‚ö° PyTorch
- Tareas: redes neuronales, aceleraci√≥n en CPU.
- Librer√≠as: `PyTorch`, `daal4py`, `oneAPI DAL`.

### üî• PySpark
- Tareas: procesamiento distribuido, MLlib.
- Librer√≠as: `PySpark`, `MLlib`, `sparksql-magic`.

### üß¨ TensorFlow
- Tareas: entrenamiento y despliegue de modelos de deep learning.
- Librer√≠as: `TensorFlow`, `TensorBoard`, `ADS`.

---

## 7. Conclusi√≥n

Los entornos Conda en OCI Data Science son una soluci√≥n modular, escalable y decorativa para acelerar proyectos de ciencia de datos. Permiten trabajar con herramientas de √∫ltima generaci√≥n sin preocuparse por la instalaci√≥n manual, y est√°n dise√±ados para adaptarse a distintos casos de uso, arquitecturas y versiones de Python.

---

## üß† Librer√≠as mencionadas en entornos Conda de OCI Data Science

| üìö Librer√≠a              | üß© Caracter√≠stica principal                                                                 | üóÇÔ∏è Categor√≠a del entorno Conda                     |
|--------------------------|---------------------------------------------------------------------------------------------|----------------------------------------------------|
| **oracle-ads (ADS SDK)** | SDK para ML en OCI: ingesti√≥n, AutoML, explicabilidad, despliegue.                         | Todos los entornos                                 |
| **OCI Python SDK**       | Acceso a servicios de OCI desde Python.                                                    | Todos los entornos                                 |
| **scikit-image**         | Procesamiento de im√°genes.                                                                 | Computer Vision                                    |
| **Pillow**               | Manipulaci√≥n de im√°genes.                                                                  | Computer Vision                                    |
| **PyTorch**              | Deep Learning con soporte GPU.                                                             | Computer Vision, NLP, General ML                   |
| **OpenCV**               | Algoritmos de visi√≥n por computadora.                                                      | Computer Vision                                    |
| **Kafka (Python)**       | Consumo de streams de datos.                                                               | Data Exploration & Manipulation                    |
| **pandas**               | Manipulaci√≥n de datos tabulares.                                                           | Data Exploration & Manipulation, General ML        |
| **pandaparallel**        | Paralelizaci√≥n de pandas.                                                                  | Data Exploration & Manipulation                    |
| **task**                 | (No especificado, posiblemente auxiliar).                                                  | Data Exploration & Manipulation                    |
| **Matplotlib**           | Visualizaci√≥n 2D.                                                                          | Data Exploration & Manipulation                    |
| **Seaborn**              | Visualizaci√≥n estad√≠stica.                                                                 | Data Exploration & Manipulation                    |
| **Plotly**               | Gr√°ficos interactivos.                                                                     | Data Exploration & Manipulation                    |
| **Bokeh**                | Visualizaci√≥n web interactiva.                                                             | Data Exploration & Manipulation                    |
| **xgboost**              | Algoritmo de boosting eficiente.                                                           | General Machine Learning                           |
| **lightgbm**             | Boosting r√°pido y ligero.                                                                  | General Machine Learning                           |
| **Keras con TensorFlow** | API de alto nivel para redes neuronales.                                                   | General Machine Learning                           |
| **Oracle MLX**           | Explicabilidad de modelos.                                                                 | General Machine Learning                           |
| **nltk**                 | Procesamiento de texto.                                                                    | Natural Language Processing                        |
| **keybert**              | Extracci√≥n de frases clave con BERT.                                                       | Natural Language Processing                        |
| **pytorch-lightning**    | Abstracci√≥n de PyTorch para NLP.                                                           | Natural Language Processing                        |
| **simpletransformers**   | Framework NLP con transformers.                                                            | Natural Language Processing                        |
| **ONNX**                 | Formato portable de modelos ML.                                                            | ONNX (Interoperabilidad y despliegue)              |
| **ONNX Runtime**         | Ejecuci√≥n de modelos ONNX.                                                                 | ONNX (Interoperabilidad y despliegue)              |
| **ipython-sql**          | Comandos SQL en notebooks.                                                                 | Oracle Database                                    |
| **SQLAlchemy**           | ORM para bases de datos.                                                                   | Oracle Database                                    |
| **daal4py**              | Aceleraci√≥n de scikit-learn en CPUs Intel.                                                 | PyTorch (optimizado para CPU)                      |
| **oneAPI DAL**           | Librer√≠a de Intel para an√°lisis de datos.                                                  | PyTorch (optimizado para CPU)                      |
| **PySpark**              | API Python para Apache Spark.                                                              | PySpark (procesamiento distribuido)                |
| **MLlib**                | ML en Spark.                                                                               | PySpark                                            |
| **sparksql-magic**       | Comandos Spark SQL en notebooks.                                                           | PySpark                                            |
| **TensorFlow**           | Framework de ML y Deep Learning.                                                           | TensorFlow                                         |
| **TensorBoard**          | Visualizaci√≥n de m√©tricas de entrenamiento.                                                | TensorFlow                                         |



</br>
</br>
---

# üß† Gesti√≥n de entornos Conda con la herramienta de l√≠nea de comandos `odsc`

## 1. Introducci√≥n
En este m√≥dulo aprender√°s a **gestionar entornos Conda** utilizando la herramienta de l√≠nea de comandos `odsc` incluida en el servicio **Oracle Cloud Infrastructure (OCI) Data Science**.

Antes de entrar en detalle, recordemos qu√© es un entorno Conda:  
Un **entorno Conda** es una colecci√≥n de software empaquetada que incluye int√©rprete, librer√≠as y configuraciones necesarias para ejecutar proyectos de ciencia de datos.  
En OCI existen:
- **Conda packs gestionados** por el equipo de Data Science Service.
- **Conda packs publicados por el usuario**, que pod√©s crear y compartir.

Aunque gran parte de la gesti√≥n se puede hacer con la interfaz gr√°fica **Environment Explorer**, la CLI `odsc` ofrece **mayor control y flexibilidad**.

---

## 2. Funcionalidades principales de `odsc conda`

Con la CLI pod√©s:

- **Explorar (browse)**: ver informaci√≥n detallada de entornos Conda en formato YAML (nombre, descripci√≥n, librer√≠as clave, *slug*).
- **Buscar (search)**: combinar `odsc conda list` con herramientas como `grep` para filtrar resultados.
- **Instalar (install)**: a√±adir entornos Conda gestionados o publicados.
- **Clonar (clone)**: copiar un entorno a otra ubicaci√≥n para modificarlo sin afectar el original.
- **Modificar (modify)**: actualizar o cambiar librer√≠as de un entorno instalado.
- **Publicar (publish)**: guardar entornos personalizados en Object Storage para compartirlos o usarlos en despliegues y *jobs*.
- **Eliminar (delete)**: borrar entornos que ya no necesites.
- **Crear (create)**: generar entornos personalizados a partir de un archivo `environment.yaml`.

---

## 3. Exploraci√≥n de entornos Conda

Pod√©s hacerlo de dos maneras:
1. **Environment Explorer** (GUI).
2. **L√≠nea de comandos** con:
   ```bash
   odsc conda list
   ```
   - Por defecto, muestra entornos **Data Science** gestionados.
   - Para listar solo los entornos instalados localmente en tu notebook:
     ```bash
     odsc conda list --local
     ```
   - Para listar entornos **publicados**:
     ```bash
     odsc conda list --override
     ```
     *(Requiere tener configurado el bucket de Object Storage y `odsc conda init`)*

---

## 4. B√∫squeda avanzada

No existe un comando espec√≠fico de b√∫squeda en la CLI, pero pod√©s filtrar resultados combinando `odsc conda list` con utilidades como `grep`, `awk` o `perl`.

Ejemplo para obtener nombres y *slugs*:
```bash
odsc conda list | grep -E "name:|slug:"
```

---

## 5. Instalaci√≥n de entornos Conda

- **Instalar un entorno gestionado**:
  ```bash
  odsc conda install --slug <nombre-del-slug>
  ```
- **Instalar un entorno publicado**:
  ```bash
  odsc conda install --slug <nombre-del-slug> --override
  ```
  *(El par√°metro `--override` indica que se debe instalar desde entornos publicados en Object Storage, no desde los gestionados por OCI)*

---

## 6. Buenas pr√°cticas

- **Clonar antes de modificar**: evita romper un entorno funcional.
- **Publicar entornos estables**: facilita la colaboraci√≥n y el despliegue.
- **Usar nombres descriptivos en el slug**: mejora la b√∫squeda y el mantenimiento.
- **Documentar cambios**: registra librer√≠as a√±adidas o actualizadas.

---

# üß† Gesti√≥n avanzada de entornos Conda con `odsc` (Parte 2)

## 7. Clonar entornos Conda

La clonaci√≥n permite **copiar e instalar** un entorno Conda existente, creando una **nueva versi√≥n personalizada** sin modificar el original.

**Comando base:**
```bash
odsc conda clone --from <slug_origen> --env <nombre_nuevo_entorno>
```

- **`--from`**: *slug* del entorno Conda instalado que quer√©s clonar.
- **`--env`**: nombre del nuevo entorno.  
  Al asignarlo, OCI genera autom√°ticamente un nuevo *slug*.

üí° **Ventaja**: Ideal para probar cambios importantes sin arriesgar la estabilidad del entorno original.

---

## 8. Modificar un entorno Conda

Cuando quer√©s **instalar nuevas librer√≠as** o **actualizar versiones**, no se usa `odsc`, sino la herramienta est√°ndar `conda`.

**Pasos:**
8.1. Activar el entorno:
   ```bash
   conda activate /home/datascience/conda/<slug_entorno>
   ```
8.2. Realizar cambios. Ejemplo: actualizar ADS a la √∫ltima versi√≥n:
   ```bash
   python3 -m pip install oracle-ads --upgrade
   ```

üîπ Los cambios afectan **solo** al entorno activado.

---

## 9. Publicar un entorno Conda

Publicar un entorno lo **empaqueta y sube a un bucket de Object Storage**, permitiendo:
- Compartirlo con tu equipo.
- Usarlo en *jobs* y despliegues de modelos.
- Garantizar **reproducibilidad**.

**Preparaci√≥n:**
9.1. Crear un bucket en **Object Storage**.
9.2. Anotar:
   - **Namespace** del bucket.
   - **Nombre** del bucket.
9.3. Inicializar la configuraci√≥n (solo una vez por sesi√≥n):
   ```bash
   odsc conda init --bucket_namespace <namespace> --bucket_name <nombre_bucket>
   ```

**Publicar:**
```bash
odsc conda publish --slug <slug_entorno>
```

---

## 10. Eliminar un entorno Conda

Para liberar espacio:
```bash
odsc conda delete --slug <slug_entorno>
```

---

## 11. Crear un entorno Conda desde cero

Pod√©s crear un entorno nuevo usando un **archivo de manifiesto Conda** (`environment.yaml`).

**Comando:**
```bash
odsc conda create --file <ruta/environment.yaml>
```

- Por defecto, se instalan paquetes base desde `/opt/base-env.yaml` para compatibilidad con JupyterLab.
- Si quer√©s evitarlo:
  ```bash
  odsc conda create --file <ruta/environment.yaml> --empty
  ```
  ‚ö†Ô∏è No recomendado, ya que podr√≠a impedir que el entorno aparezca como kernel en JupyterLab.

---

## 12. Resumen de funcionalidades vistas

| Acci√≥n        | Comando principal                         | Uso t√≠pico |
|---------------|-------------------------------------------|------------|
| **Explorar**  | `odsc conda list`                         | Ver entornos disponibles |
| **Buscar**    | `odsc conda list | grep ...`               | Filtrar por nombre o slug |
| **Instalar**  | `odsc conda install --slug ...`            | A√±adir entornos gestionados o publicados |
| **Clonar**    | `odsc conda clone --from ... --env ...`    | Crear copia editable |
| **Modificar** | `conda activate ...` + `pip install ...`   | Actualizar librer√≠as |
| **Publicar**  | `odsc conda publish --slug ...`            | Compartir en Object Storage |
| **Eliminar**  | `odsc conda delete --slug ...`             | Borrar entornos |
| **Crear**     | `odsc conda create --file ...`             | Nuevo entorno desde YAML |

üìå **Resumen**:  
La CLI `odsc` es la herramienta m√°s potente para gestionar entornos Conda en OCI. Te permite no solo instalar y explorar, sino tambi√©n clonar, modificar, publicar y crear entornos personalizados, asegurando control total sobre tu infraestructura de ciencia de datos.



</br>
</br>

---

# üß† Demo: Gesti√≥n de entornos Conda con la CLI `odsc`

## 1. Introducci√≥n

En esta demostraci√≥n aprender√°s a **gestionar entornos Conda** usando la herramienta de l√≠nea de comandos `odsc` en **OCI Data Science**.  
Veremos c√≥mo:

- Explorar (*browse*)
- Buscar (*search*)
- Instalar (*install*)
- Clonar (*clone*)
- Modificar (*modify*)
- Publicar (*publish*)
- Eliminar (*delete*) entornos Conda
- Crear un entorno personalizado desde cero usando un archivo **YAML**

El comando principal que utilizaremos es:

```bash
odsc
```

Al ejecutar `odsc --help`, ver√°s varios subcomandos. En este caso, trabajaremos con el subcomando:

```bash
odsc conda
```

---

## 2. Subcomandos disponibles

Al listar la ayuda de `odsc conda`, encontrar√°s opciones como:

- `list` ‚Üí listar entornos
- `initialize` ‚Üí inicializar configuraci√≥n
- `show-configuration` ‚Üí mostrar configuraci√≥n actual
- `publish` ‚Üí publicar entornos
- `install` ‚Üí instalar entornos
- ‚Ä¶ entre otros

---

## 3. Explorando entornos Conda

### 3.1 Listar entornos gestionados por OCI
```bash
odsc conda list
```
Devuelve un archivo **YAML** con informaci√≥n detallada:
- **name** ‚Üí nombre del conda-pack
- **slug** ‚Üí identificador √∫nico
- **type** ‚Üí `dataScience` (gestionado por OCI)

### 3.2 Listar entornos instalados localmente
```bash
odsc conda list --local
```
- Muestra entornos instalados en la sesi√≥n de notebook.
- El campo **type** ser√° `local`.

### 3.3 Listar entornos publicados
```bash
odsc conda list --override
```
- Muestra entornos publicados en Object Storage.
- Requiere configuraci√≥n previa (`odsc conda init`).

---

## 4. B√∫squeda de informaci√≥n

El listado en YAML puede ser extenso. Para filtrar, combinamos `odsc conda list` con herramientas como `grep` y expresiones regulares.

Ejemplo: obtener solo nombres y slugs
```bash
odsc conda list | grep -E "name:|slug:"
```

Para buscar entornos relacionados con *Data Exploration*:
```bash
odsc conda list | grep -i "data exploration"
```

---

## 5. Instalaci√≥n de un entorno Conda

Ejemplo: instalar un entorno de *Data Exploration*:

1. Buscar el slug:
   ```bash
   odsc conda list | grep -i "data exploration"
   ```
2. Instalar:
   ```bash
   odsc conda install --slug <slug_encontrado>
   ```
3. El sistema pedir√° la versi√≥n a instalar.
4. Descargar√° y extraer√° el entorno.

Verificar instalaci√≥n:
```bash
odsc conda list --local | grep <slug>
```

---

## 6. Clonaci√≥n de un entorno Conda

Para modificar un entorno sin afectar el original:

```bash
odsc conda clone --from <slug_origen> --env "Mi Data Exploration"
```

- `--from`: slug del entorno original.
- `--env`: nombre del nuevo entorno (OCI generar√° un slug autom√°ticamente).
- El sistema pedir√° la versi√≥n y luego empaquetar√° e instalar√° la copia.

---

## 7. Flujo de trabajo t√≠pico en esta demo

1. **Listar** entornos gestionados (`odsc conda list`).
2. **Filtrar** por nombre o slug con `grep`.
3. **Instalar** el entorno deseado (`odsc conda install`).
4. **Verificar** instalaci√≥n (`odsc conda list --local`).
5. **Clonar** para personalizar (`odsc conda clone`).

---
# üß† Gesti√≥n avanzada de entornos Conda con `odsc` (Parte final del cap√≠tulo)

## 8. Verificar el *slug* del entorno clonado

Despu√©s de clonar un entorno Conda, es importante identificar su **slug** (identificador √∫nico).  
Para hacerlo:

```bash
odsc conda list --local | grep -E "name:|slug:"
```

- `--local` ‚Üí lista solo los entornos instalados en la sesi√≥n de notebook.
- `grep` ‚Üí filtra para mostrar √∫nicamente los campos `name` y `slug`.

---

## 9. Modificar un entorno Conda

Para modificar un entorno (a√±adir librer√≠as, cambiar versiones, etc.) **no** se usa `odsc conda`, sino la herramienta est√°ndar `conda`.

**Pasos:**
1. Activar el entorno:
   ```bash
   conda activate /home/datascience/conda/<slug_entorno>
   ```
2. Realizar cambios. Ejemplo: instalar la librer√≠a `pendulum`:
   ```bash
   python3 -m pip install pendulum
   ```
3. Desactivar el entorno:
   ```bash
   conda deactivate
   ```

üí° **Nota:** Los cambios afectan √∫nicamente al entorno activado.

---

## 10. Publicar un entorno Conda

Publicar un entorno lo empaqueta y lo sube a un **bucket de Object Storage**, permitiendo:
- Compartirlo con otros usuarios.
- Usarlo en *jobs* y despliegues de modelos.
- Garantizar reproducibilidad.

**Pasos:**
1. Crear un bucket en Object Storage (ej. `published-conda-environments`).
2. Anotar:
   - **Namespace** del bucket.
   - **Nombre** del bucket.
3. Inicializar la configuraci√≥n (solo una vez por sesi√≥n):
   ```bash
   odsc conda init --bucket_namespace <namespace> --bucket_name <nombre_bucket>
   ```
4. Publicar el entorno:
   ```bash
   odsc conda publish --slug <slug_entorno>
   ```

---

## 11. Verificar publicaci√≥n

Para confirmar que el entorno est√° publicado:
```bash
odsc conda list --override | grep -E "name:|slug:"
```
- `--override` ‚Üí lista entornos publicados en Object Storage en lugar de los gestionados por OCI.

En el bucket, la estructura ser√°:
```
Conda Environments/
   CPU/
      My Data Exploration/
         v1/
            <archivos del entorno>
```

---

## 12. Eliminar un entorno Conda

Para liberar espacio en la sesi√≥n de notebook:
```bash
odsc conda delete --slug <slug_entorno>
```
El sistema pedir√° confirmaci√≥n antes de eliminarlo.

---

## 13. Crear un entorno Conda desde un archivo YAML

Otra forma de crear un entorno es a partir de un **archivo de manifiesto Conda** (`environment.yaml`).

**Comando:**
```bash
odsc conda create --file <ruta/environment.yaml>
```

- Por defecto, se a√±aden paquetes base desde `/opt/base-env.yaml` para compatibilidad con JupyterLab.
- Para evitarlo:
  ```bash
  odsc conda create --file <ruta/environment.yaml> --empty
  ```
  ‚ö†Ô∏è No recomendado, ya que podr√≠a impedir que el entorno aparezca como kernel en JupyterLab.

---

üìå **Resumen**:  
Esta demo muestra c√≥mo usar `odsc conda` para explorar, buscar, instalar y clonar entornos Conda en OCI Data Science, combinando la potencia de la CLI con utilidades de filtrado como `grep` para trabajar de forma m√°s √°gil y precisa.


Resumen de operaciones vistas
| Acci√≥n        | Comando principal                         | Uso t√≠pico |
|---------------|-------------------------------------------|------------|
| **Listar**    | `odsc conda list`                         | Ver entornos disponibles |
| **Buscar**    | `odsc conda list | grep ...`               | Filtrar por nombre o slug |
| **Instalar**  | `odsc conda install --slug ...`            | A√±adir entornos gestionados o publicados |
| **Clonar**    | `odsc conda clone --from ... --env ...`    | Crear copia editable |
| **Modificar** | `conda activate ...` + `pip install ...`   | Actualizar librer√≠as |
| **Publicar**  | `odsc conda publish --slug ...`            | Compartir en Object Storage |
| **Eliminar**  | `odsc conda delete --slug ...`             | Borrar entornos |
| **Crear**     | `odsc conda create --file ...`             | Nuevo entorno desde YAML |



</br>
</br>
---
# üß† OCI Vault: Gesti√≥n segura de secretos y claves para Data Science

## 1. Introducci√≥n

En este m√≥dulo aprender√°s:

- Por qu√© es importante usar el servicio **OCI Vault** para compartir y proteger secretos.
- Conceptos clave como:
  - Tipos de claves.
  - Rotaci√≥n de claves.
  - Almacenamiento seguro de secretos en el Vault (en lugar de en tu c√≥digo).

---

## 2. ¬øQu√© es OCI Vault y por qu√© es importante?

En un flujo de trabajo de ciencia de datos, es habitual interactuar con otros servicios para:

- Conectarse a datos importantes.
- Almacenar y recuperar artefactos.
- Importar y exportar informaci√≥n.

Muchas de estas interacciones requieren **credenciales** (usuarios, contrase√±as, tokens, certificados).

**Mejor pr√°ctica de seguridad:**  
Nunca almacenar credenciales localmente o en el c√≥digo, ya que es una de las formas m√°s comunes de filtraci√≥n.  
En su lugar, se recomienda usar un servicio especializado: **OCI Vault**.

---

## 3. Funcionalidad principal de OCI Vault

- Servicio **gestionado por Oracle** para administrar de forma centralizada:
  - **Claves de cifrado**.
  - **Credenciales** y secretos.
- Soporta tres algoritmos de cifrado:
  - **AES** (Advanced Encryption Standard)
  - **RSA** (Rivest‚ÄìShamir‚ÄìAdleman)
  - **ECDSA** (Elliptic Curve Digital Signature Algorithm)
- Integraci√≥n con:
  - **OCI SDK**
  - **CLI de OCI**
  - **Clientes API**
  - **ADS SDK** (con m√∫ltiples puntos de integraci√≥n entre Data Science y otros servicios OCI)

Esto facilita almacenar credenciales en el Vault y conectarse a los recursos necesarios sin exponer datos sensibles en c√≥digo o archivos de configuraci√≥n.

---

## 4. Elementos principales del servicio

- **Vaults**: contenedores l√≥gicos para almacenar claves y secretos.
- **Keys**: entidades l√≥gicas que representan una o m√°s versiones de material criptogr√°fico.
- **Secrets**: credenciales o datos sensibles cifrados.

El Vault elimina la necesidad de guardar claves y secretos en archivos de configuraci√≥n o en el c√≥digo.

---

## 5. Mejores pr√°cticas

Tanto ingenieros DevOps como cient√≠ficos de datos deber√≠an usar el Vault para **mejorar la postura de seguridad** de sus aplicaciones y servicios.

---

## 6. Tipos de Vault

Al crear un Vault, se puede elegir entre:

### üîπ Virtual Private Vault
- Partici√≥n dedicada en un **HSM** (*Hardware Security Module*).
- Hasta **1.000 versiones de claves**.
- Mejor aislamiento.
- Permite **copias de seguridad** en Object Storage.
- Soporta **recuperaci√≥n ante desastres** y **replicaci√≥n entre regiones**.

### üîπ Vault en partici√≥n compartida (opci√≥n por defecto)
- Comparte la partici√≥n HSM con otros clientes de Oracle.
- Seguridad garantizada, pero sin funciones avanzadas como backup a Object Storage.
- **Menor coste**: se cobra solo por el n√∫mero de claves, versiones y secretos almacenados.

---

## 7. ¬øQu√© son las claves (*Keys*)?

- Entidades l√≥gicas que representan una o m√°s versiones de material criptogr√°fico.
- El material criptogr√°fico se genera para un algoritmo espec√≠fico (AES, RSA o ECDSA).
- Se utilizan para:
  - **Cifrar datos**.
  - **Firmar digitalmente** informaci√≥n.


---

# üîê OCI Vault ‚Äì Tipos de claves, rotaci√≥n y gesti√≥n de secretos

## 8. T√©cnicas de cifrado y descifrado

Las tecnolog√≠as de cifrado utilizan distintos enfoques:

- **AES** ‚Üí Cifrado sim√©trico: la misma clave puede cifrar y descifrar los datos.
- **RSA** y **ECDSA** ‚Üí Cifrado asim√©trico: una clave cifra y la otra descifra.

---

## 9. Tipos de claves en OCI Vault

En el servicio Vault existen tres tipos conceptuales de claves:

### 9.1 Master Encryption Key (Clave maestra de cifrado)
- Claves que cre√°s o import√°s en tu Vault.
- Defin√≠s:
  - Algoritmo de cifrado (AES, RSA, ECDSA).
  - Forma de la clave (*key shape*): n√∫mero de bits (AES/RSA) o ID de curva el√≠ptica (ECDSA).

### 9.2 Data Encryption Key (Clave de cifrado de datos)
- Generadas a partir de una clave maestra.
- Se cifran din√°micamente con la clave maestra (**envelope encryption**).
- Permiten:
  - Usar m√∫ltiples claves para distintos datos.
  - Rotar claves sin afectar todo el sistema.
  - Reducir el impacto si una clave se ve comprometida (solo afecta a los datos cifrados con ella).

### 9.3 Wrapping Keys
- Usadas para proteger (envolver) otras claves durante su transporte o almacenamiento.

---

## 10. Uso de claves en servicios OCI

Servicios como **Block Storage** o **File Storage** pueden:
- Usar claves gestionadas por Oracle.
- Usar claves maestras personalizadas desde tu Vault.

Ejemplo: un bucket de Object Storage o un volumen de bloque puede usar una clave de cifrado de datos espec√≠fica.

---

## 11. Rotaci√≥n de claves maestras

- Cada clave tiene una **versi√≥n** asignada al crearse.
- Al rotar:
  - Se genera autom√°ticamente una nueva versi√≥n.
  - O pod√©s importar otra clave.
- **Mejor pr√°ctica**: rotar peri√≥dicamente para limitar la cantidad de datos cifrados con una misma versi√≥n.
- El Vault usa el **OCID** de la versi√≥n anterior para descifrar datos cifrados con ella.
  - Versiones antiguas ‚Üí ya no cifran, pero s√≠ pueden descifrar.

---

## 12. Componentes clave del servicio Vault

1. **Keys** ‚Üí Claves de cifrado.
2. **HSM** (*Hardware Security Module*) con certificaci√≥n **FIPS 140-2 Nivel 3**.
3. **Secrets** ‚Üí Datos sensibles como contrase√±as, tokens o credenciales.

---

## 13. Gesti√≥n de secretos

- **Qu√© son**: credenciales necesarias para acceder a servicios OCI o aplicaciones externas.
- **Ventajas de almacenarlos en Vault**:
  - Mayor seguridad que en c√≥digo o archivos de configuraci√≥n.
  - Recuperaci√≥n bajo demanda.
  - C√≥digo m√°s robusto: si cambian las credenciales, solo se actualizan en Vault.
- **Funcionamiento**:
  - Guard√°s localmente el **OCID** del secreto.
  - El c√≥digo solicita las credenciales al Vault cuando las necesita.
  - Se usan y luego se descartan.
- **Control de acceso**:
  - M√°s f√°cil restringir acceso al secreto que al notebook.
- **Creaci√≥n de secretos**:
  - Desde la **Consola OCI**.
  - Program√°ticamente con **OCI SDK**, **CLI** o **REST API**.
- **Versionado**:
  - Cada secreto tiene un OCID √∫nico que no cambia.
  - Al rotar, se crea una nueva versi√≥n con el contenido actualizado.
  - Rotar peri√≥dicamente reduce el impacto de una posible exposici√≥n.

---

## Resumen del m√≥dulo

En este cap√≠tulo aprendiste:

OCI Vault es la herramienta centralizada para almacenar y gestionar claves y secretos de forma segura en OCI. Protege credenciales, evita filtraciones y se integra con m√∫ltiples servicios y SDKs, ofreciendo opciones de aislamiento y recuperaci√≥n seg√∫n las necesidades y presupuesto.

- Qu√© es el servicio **OCI Vault** y su importancia.
- Tipos de Vault y tipos de claves.
- Concepto de versiones y rotaci√≥n de claves.
- Gesti√≥n segura de secretos y credenciales.
- Por qu√© **no** almacenar credenciales en c√≥digo, sino en el Vault.



</br>
</br>

---
# üîê Gesti√≥n de cifrado y secretos en OCI: Oracle Managed Keys vs Customer Managed Keys

## 1. Introducci√≥n

En este m√≥dulo aprender√°s:

- Las distintas formas de gestionar el cifrado en OCI:
  - **Oracle Managed Keys** (claves gestionadas por Oracle).
  - **Customer Managed Keys** (claves gestionadas por el cliente).
- C√≥mo usar el **OCI SDK** para almacenar y recuperar secretos desde el Vault.
- Clases especializadas del **ADS SDK** dise√±adas para integrarse con el Vault y simplificar el acceso a recursos seguros en flujos de trabajo de ciencia de datos.

---

## 2. Cifrado en OCI

OCI utiliza cifrado en m√∫ltiples puntos del servicio.  
Al trabajar con recursos, se te pedir√° elegir entre:

- **Oracle Managed Keys** ‚Üí Claves maestras gestionadas por Oracle en su Vault interno.
- **Customer Managed Keys** ‚Üí Claves maestras almacenadas en tu propio Vault.

### 2.1 Oracle Managed Keys
- Por defecto, OCI cifra y descifra datos usando claves maestras gestionadas por Oracle.
- Ejemplo: al aprovisionar un **Block Volume**, **Object Storage Bucket** o un **OKE Cluster**, Oracle usa una de sus claves maestras para generar la **Data Encryption Key** que cifrar√° los datos.

### 2.2 Customer Managed Keys
- La clave maestra est√° en tu Vault.
- Se usa para generar las **Data Encryption Keys**.
- Aunque no uses el Vault, **todos los datos en reposo (data at rest)** en OCI est√°n cifrados por defecto y no se puede desactivar.
- Obligatorio en ciertos contextos, como **Security Zones**, donde no se permite usar claves gestionadas por Oracle.

üí° **Nota**: *Customer Managed Key* no es lo mismo que *Bring Your Own Key (BYOK)*.  
En este caso, la clave puede haber sido:
- Importada previamente a tu Vault.
- Generada directamente por el servicio Vault.

En ambos casos:
- La clave es tuya.
- Gestion√°s su rotaci√≥n.
- Sos responsable de su ciclo de vida.

---

## 3. Recursos que requieren elecci√≥n de tipo de clave

- Block Volumes y Boot Volumes.
- File Systems.
- Object Storage.
- Kubernetes Secrets (OKE).
- Autonomous Container Databases.
- OCI Streaming Pools.
- Y m√°s.

---

## 4. Configuraci√≥n de una Customer Managed Key

1. Seleccionar la opci√≥n **Customer Managed Key**.
2. Localizar tu Vault.
3. Elegir la clave maestra que deseas usar.
4. Esa clave generar√° las **Data Encryption Keys** que cifrar√°n los datos.

---

## 5. Trabajo con secretos en Python

### 5.1 Opciones
- **OCI SDK**:
  - API general para trabajar con el Vault.
  - Muy potente, pero m√°s compleja.
  - No est√° pensada espec√≠ficamente para flujos de trabajo de ciencia de datos.
- **ADS SDK**:
  - Clases adaptadas a Data Science.
  - M√°s simple para casos de uso comunes de cient√≠ficos de datos.

---

## 6. Ejemplo: creaci√≥n y almacenamiento de credenciales

**Problema com√∫n**: credenciales almacenadas en c√≥digo o archivos de configuraci√≥n ‚Üí inseguro y dif√≠cil de mantener.

**Ejemplo motivador**: credenciales para conectarse a una **Autonomous Database** (o cualquier base de datos).

6.1. Crear un diccionario Python con:
   - Nombre de la base de datos.
   - Usuario.
   - Contrase√±a.
   ```python
   credentials = {
       "dbname": "...",
       "username": "...",
       "password": "..."
   }
   ```

6.2. **Vault y clave ya creados**.

6.3. **Codificaci√≥n Base64**:
   - No se puede almacenar un objeto binario (diccionario Python) directamente en el Vault.
   - Convertir a JSON y luego codificar en Base64.
   - Usar `Base64SecretContentDetails`.

6.4. **Creaci√≥n del objeto de detalles del secreto**:
   - Incluir:
     - Compartment OCID.
     - Vault ID.
     - Key ID.
     - Descripci√≥n.
     - Nombre del secreto.
     - Contenido codificado en Base64.

---

## 7. Almacenamiento del secreto en el Vault

7.1. Cargar configuraci√≥n OCI:
   ```python
   config = oci.config.from_file()
   ```
7.2. Crear cliente de Vault:
   ```python
   vault_client = oci.vault.VaultsClient(config)
   ```
7.3. Usar `vaults_client_composite_operations` para:
   - Llamar a `create_secret_and_wait_for_state`.
   - Pasar el objeto de detalles del secreto.
   - Esperar hasta que el estado sea **ACTIVE**.

---

# üîê Recuperaci√≥n de secretos y uso del ADS SDK con OCI Vault

## 8. Recuperar secretos desde el Vault con OCI SDK

Para recuperar secretos almacenados en el Vault usando el **OCI SDK**:

8.1. **Cliente de secretos**:
   - Usar la clase `SecretsClient`.
   - Requiere el objeto de configuraci√≥n (`config`) que ya cargamos previamente.

8.2. **M√©todo principal**:
   - `get_secret_bundle(secret_ocid)` ‚Üí devuelve un **response object**.

8.3. **Estructura de acceso**:
   - `response.data` ‚Üí devuelve un **secret bundle**.
   - `secret_bundle_content` ‚Üí contiene el objeto con el contenido codificado en Base64.
   - `content` ‚Üí el valor real del secreto en Base64.

8.4. **Decodificaci√≥n**:
   - Crear un m√©todo auxiliar que:
     - Decodifique Base64.
     - Convierta el JSON resultante en un diccionario Python.

**Flujo resumido**:
```python
# 1. Crear cliente de secretos
secret_client = oci.secrets.SecretsClient(config)

# 2. Obtener el bundle
secret_bundle = secret_client.get_secret_bundle(secret_ocid)

# 3. Extraer y decodificar
decoded_secret = base64_to_dict(secret_bundle.data.secret_bundle_content.content)
```

---

## 9. Uso del ADS SDK para simplificar la gesti√≥n de secretos

El **ADS SDK** ofrece clases especializadas para almacenar y recuperar secretos en OCI Vault, pensadas para flujos de trabajo de ciencia de datos:

- **ADBSecretKeeper** ‚Üí Autonomous Database (incluye opci√≥n de almacenar el *wallet file*).
- **BDSSecretKeeper** ‚Üí OCI Big Data Service (HDFS, Hive, Kerberos).
- **MySQLDBSecretKeeper** ‚Üí Oracle MySQL Database.
- **AuthTokenSecretKeeper** ‚Üí Tokens de autenticaci√≥n (ej. Streaming, GitHub).

---

## 10. Ejemplo: MySQLDBSecretKeeper

### 10.1 Guardar credenciales
```python
from ads.secrets import MySQLDBSecretKeeper

# Crear objeto
keeper = MySQLDBSecretKeeper(
    vault_id="<Vault_OCID>",
    key_id="<Key_OCID>",
    credentials={
        "dbname": "...",
        "host": "...",
        "port": 3306,
        "username": "...",
        "password": "..."
    }
)

# Guardar en el Vault
keeper.save(name="MySQL_Creds", description="Credenciales MySQL de producci√≥n")
```

### 10.2 Recuperar credenciales (mejor pr√°ctica)
Usar un **context manager** para no dejar las credenciales en memoria m√°s tiempo del necesario:
```python
with MySQLDBSecretKeeper() as keeper:
    creds = keeper.load_secret(secret_ocid="<Secret_OCID>")
    # creds es un diccionario con las credenciales
```

---

## 11. Ventajas del ADS SDK frente al OCI SDK puro

- **Menos c√≥digo**: guardar ‚Üí 2 l√≠neas, recuperar ‚Üí 1 l√≠nea.
- **Adaptado a casos de uso de Data Science**.
- **Soporte para m√∫ltiples servicios** (ADB, BDS, MySQL, tokens).
- **Manejo simplificado de formatos** (no requiere codificaci√≥n/decodificaci√≥n manual en Base64).

---

## 12. Resumen del m√≥dulo

En este cap√≠tulo aprendiste que:

- Entendiste la diferencia entre claves gestionadas por Oracle y por el cliente.
- Viste c√≥mo elegir y configurar una Customer Managed Key.
- Aprendiste a crear y almacenar credenciales seguras en el Vault usando Python y el OCI SDK.
- Los servicios que usan cifrado pueden tener su clave maestra gestionada por Oracle o por el cliente en su Vault.
- Las credenciales pueden convertirse de diccionario Python ‚Üí JSON ‚Üí Base64 y almacenarse en el Vault con el OCI SDK.
- El proceso con OCI SDK implica:
  - Crear `VaultsClient` y `SecretsClient`.
  - Usar `create_secret_and_wait_for_state` para almacenar.
  - Usar `get_secret_bundle` para recuperar.
  - Decodificar Base64 y reconstruir el diccionario.
- El **ADS SDK** simplifica enormemente este flujo con clases como `MySQLDBSecretKeeper`, `ADBSecretKeeper`, `BDSSecretKeeper` y `AuthTokenSecretKeeper`.



</br>
</br>

---

# üìÇ Sistemas de Control de Versiones en Ciencia de Datos (Parte 1)

## 1. Introducci√≥n

En este m√≥dulo veremos:

- Qu√© es un **sistema de control de versiones** (*Version Control System*, VCS).
- C√≥mo se utilizan en ciencia de datos.
- C√≥mo configurar uno y ejecutar comandos b√°sicos.

Los sistemas de control de versiones, tambi√©n llamados **sistemas de gesti√≥n de c√≥digo fuente** (*Source Code Management Systems*), permiten gestionar distintas versiones de c√≥digo, documentos, datos, an√°lisis y otros recursos similares.  
Aunque fueron creados para el desarrollo de software tradicional, los cient√≠ficos de datos los han adoptado para gestionar y versionar sus an√°lisis.

---

## 2. Uso en ciencia de datos

Ejemplo:  
Los usuarios de ciencia de datos utilizan sistemas de control de versiones para **rastrear las distintas versiones de sus notebooks de JupyterLab**. Esto facilita:

- Gestionar cambios.
- Compartir trabajo con otros.
- Mantener un historial de versiones.

Existen m√∫ltiples sistemas de repositorios: CVS, Bazaar, Subversion, Perforce, CodeCommit, Mercurial, Helix Core, entre otros.  
Sin embargo, **Git** es el m√°s popular y est√° integrado en el **OCI Data Science Service**, por lo que este m√≥dulo se centrar√° casi exclusivamente en Git.

---

## 3. Qu√© entendemos por ‚Äúc√≥digo‚Äù

En este contexto, ‚Äúc√≥digo‚Äù se refiere a **cualquier recurso que se rastree en un sistema de control de versiones**:

- C√≥digo fuente.
- Documentos.
- Informes.
- Datos recopilados.
- An√°lisis estad√≠sticos o de machine learning.
- Otros productos de trabajo.

---

## 4. Concepto de repositorio

Un sistema de control de versiones puede gestionar m√∫ltiples recursos, cada uno en un **repositorio** independiente.

üì¶ **Analog√≠a**:  
- El repositorio es como un **archivo o gabinete** que contiene un proyecto o an√°lisis.
- Cada repositorio mantiene un **historial de cambios** del c√≥digo base.

---

## 5. Funciones clave de un repositorio

- **Almacenamiento central** del c√≥digo.
- **Versionado**: seguimiento de versiones de desarrollo y de lanzamiento.
- **Colaboraci√≥n**: integraci√≥n de cambios de varios usuarios (*merge*).
- **Historial**: archivo de estados previos del c√≥digo y documentaci√≥n.
- **Ramas (branches)**: copias independientes del c√≥digo para trabajar en paralelo.
- **Commits**: puntos de guardado que permiten volver a un estado anterior.

Ejemplo:  
- Cre√°s un modelo de ML que funciona ‚Üí lo **commite√°s**.
- Prob√°s una mejora ‚Üí falla ‚Üí revert√≠s al commit anterior.

---

## 6. Tipos de sistemas de control de versiones

### üîπ Centralizados
- Un servidor central recibe todos los cambios.
- Ventajas: f√°cil de configurar, control administrativo.
- Ejemplos: Subversion, CVS, Perforce.

### üîπ Distribuidos
- No dependen de un √∫nico servidor.
- Cada desarrollador clona el repositorio completo en su m√°quina.
- Ventajas:
  - Trabajar sin conexi√≥n.
  - Mayor flexibilidad.
  - Creaci√≥n r√°pida de ramas.
  - Sin punto √∫nico de fallo.
- Ejemplos: Git, Bazaar, Mercurial.

---

## 7. Uso en ciencia de datos

Para un cient√≠fico de datos, un sistema distribuido como Git permite:

- Crear repositorios locales sin pedir permisos en un servidor central.
- Trabajar y analizar datos localmente.
- Archivar el repositorio al finalizar el proyecto.

**Limitaci√≥n**:  
En colaboraci√≥n, cada persona debe configurar su sistema para compartir cambios con cada otro miembro, lo que no escala bien en equipos grandes.

---

# üìÇ Sistemas de Control de Versiones en Ciencia de Datos (Parte 2)

## 8. Configuraci√≥n h√≠brida en sistemas distribuidos

Para evitar la complejidad de que cada miembro de un equipo se conecte con todos los dem√°s, normalmente se configura **un nodo central** al que todos env√≠an (*push*) sus cambios.  
Cuando alguien quiere las √∫ltimas actualizaciones, las descarga (*pull*) desde ese nodo.

En la pr√°ctica, los sistemas de control de versiones distribuidos no siempre funcionan de forma totalmente distribuida.  
Muchos equipos optan por un **modelo h√≠brido**: por ejemplo, usar Git (distribuido) pero con un servidor central para compartir.

---

## 9. Uso de Git en el flujo de trabajo de ciencia de datos

**Git** permite:

- Rastrear cambios en un conjunto de archivos.
- Revertir a versiones anteriores.
- Versionar no solo c√≥digo, sino tambi√©n:
  - Notebooks de JupyterLab.
  - Informes.
  - Otros productos de trabajo.

En equipos, un repositorio compartido permite que cada miembro contribuya y fusione (*merge*) sus cambios.  
Esto es clave para proyectos de ciencia de datos y construcci√≥n de modelos.

---

## 10. Ventajas de Git como sistema distribuido

- **Velocidad**: la mayor√≠a de operaciones son locales.
- **Trabajo offline**: solo se necesita conexi√≥n para *push* o *pull*.
- **Tolerancia a fallos**: incluso con un servidor central, se puede compartir directamente entre compa√±eros si el servidor cae.

---

## 11. Extensi√≥n de Git en OCI Data Science

OCI Data Science integra una **extensi√≥n de Git en JupyterLab**:

- Interfaz gr√°fica amigable.
- Acceso desde el √≠cono de Git o el men√∫ Git.
- Funciones:
  - Crear o clonar repositorios.
  - *Push* y *pull*.
  - *Stage* de cambios para commit.
  - Comparar versiones.
- Compatible con:
  - GitHub
  - Bitbucket
  - GitLab
  - OCI Code Repository
  - Instalaciones propias de Git

---

## 12. Terminolog√≠a b√°sica en Git

- **Commit**: instant√°nea del estado actual del c√≥digo.
  - Identificado por un **SHA ID** (hash √∫nico de 40 caracteres).
  - Puede tener etiquetas (*tags*) legibles.
- **Repositorio (repo)**: directorio que contiene todos los cambios de un proyecto.
  - Local o remoto.
  - Colecci√≥n de commits.
- **√Årea de trabajo (working area)**:
  - Archivos de la versi√≥n actual en la que trabaj√°s.
  - En notebooks, se almacenan en el **block storage local**.
- **Staging**:
  - Paso previo al commit.
  - Indica a Git qu√© archivos modificados se incluir√°n en la nueva versi√≥n.

---

## 13. Flujo de trabajo de un commit en Git

1. **√Årea de trabajo**: crear o modificar archivos.
2. **Staging**: seleccionar qu√© cambios incluir.
3. **Commit**:
   - Guardar cambios en el repositorio.
   - A√±adir un comentario descriptivo.
   - Registrar autor y fecha.
4. **Compartir**:
   - *Push* ‚Üí enviar cambios a otro repositorio/peer.
   - *Pull* ‚Üí traer cambios de otro repositorio/peer.

üí° **Buenas pr√°cticas**:
- Hacer commits frecuentes y peque√±os.
- No incluir en *staging* archivos irrelevantes o que no deban ser rastreados.

---

## 14. OCI Code Repository

- Servicio similar a GitHub o Bitbucket, pero integrado en OCI.
- Funciona como **peer central**.
- Integrado con el sistema de **Identity and Access Management (IAM)** de OCI.
- Permite:
  - Commits.
  - Actualizaciones.
  - Creaci√≥n de ramas (*branches*).
  - Control de acceso seguro.

---

# üìÇ Sistemas de Control de Versiones en Ciencia de Datos (Parte 3)

## 15. Gesti√≥n y visualizaci√≥n de repositorios en OCI

- En la **consola de OCI** pod√©s ver:
  - El **almacenamiento** utilizado por cada repositorio.
  - El **OCID** asignado a cada repo al crearlo.
- Con ese OCID pod√©s:
  - Editar el contenido.
  - Clonar el repo a otro peer.
  - Eliminar repos que ya no necesites.
  - Ver el historial de cambios y commits.
  - Navegar por ramas (*branches*).
  - Consultar el tama√±o del repositorio.

---

## 16. Trabajo con repositorios remotos y externos

En **OCI Data Science** pod√©s trabajar con repositorios remotos como:

- GitLab
- Bitbucket
- Tus propios repos Git

Tambi√©n pod√©s **conectar repos externos** (ej. GitHub) al **OCI Code Repository**.  
La conexi√≥n incluye:
- Tipo (GitHub, GitLab, etc.).
- Nombre.
- Referencia a un **secreto en OCI Vault** que almacena el *personal access token* de tu cuenta.

üí° **Ventaja**:  
Una vez definida la conexi√≥n, los cambios en tu repo de GitHub se **replican autom√°ticamente** en el OCI Code Repository.

---

## 17. Permisos y pol√≠ticas

Para dar acceso a repos y otros recursos, deb√©s crear **pol√≠ticas IAM**.  
Con un repositorio OCI o externo, pod√©s:

- Clonarlo v√≠a **HTTPS** o **SSH keys**.
- Crear una copia local.
- Agregar o eliminar archivos.
- Hacer commits.
- Trabajar con ramas y operaciones t√≠picas de GitHub.

---

## 18. Git vs GitHub

- **Git** ‚Üí Sistema de control de versiones.
- **GitHub** ‚Üí Servicio en la nube que aloja proyectos Git.

---

## 19. Conexi√≥n mediante SSH

- **SSH** permite conectar y autenticarte a servidores remotos sin ingresar usuario/token cada vez.
- Pasos:
  1. Instalar y configurar Git (nombre y email).
  2. Crear cuenta en GitHub (si no la ten√©s).
  3. Generar un **par de claves SSH** (privada y p√∫blica).
  4. Agregar la clave p√∫blica a GitHub.
  5. Autenticar tu repo local con GitHub.

---

## 20. Flujo b√°sico de trabajo con Git y GitHub

1. Tener un repo local y uno remoto (GitHub).
2. Si empez√°s de cero:
   - Crear repo vac√≠o en GitHub.
   - Clonarlo en tu notebook:
     ```bash
     git clone <URL>
     ```
3. Crear/modificar archivos.
4. Hacer commit en el repo local.
5. Hacer push al repo remoto para compartir cambios.

---

## 21. Comandos b√°sicos de Git

| Comando         | Funci√≥n |
|-----------------|---------|
| `git init`      | Inicializa un nuevo repo o convierte un directorio existente en repo Git. |
| `git clone`     | Crea una copia local de un repo remoto y los vincula. |
| `git add`       | A√±ade archivos al *staging area* para el pr√≥ximo commit. |
| `git commit`    | Guarda una instant√°nea de los archivos en *staging*. |
| `git remote`    | Gestiona conexiones entre repos locales y remotos. |
| `git fetch`     | Descarga commits y datos del repo remoto sin fusionar. |
| `git push`      | Sube commits locales al repo remoto. |
| `git pull`      | Descarga y fusiona cambios del repo remoto al local. |

---

## 22. Resumen del m√≥dulo

En este cap√≠tulo vimos:

- Qu√© es un sistema de control de versiones y sus tipos (centralizado y distribuido).
- Definici√≥n y usos de un repositorio de c√≥digo.
- Importancia del control de versiones en ciencia de datos.
- Extensi√≥n de Git en JupyterLab.
- Terminolog√≠a clave: commit, repo, √°rea de trabajo, staging.
- OCI Code Repository y su integraci√≥n con repos externos como GitHub.
- Pasos para configurar un repo remoto.
- Comandos esenciales de Git.



</br>
</br>
---

# üìÇ Demo: Creaci√≥n y uso de un repositorio Git local y remoto en GitHub

## 1. Objetivo de la demo
El objetivo de esta demostraci√≥n es:

- Crear un nuevo repositorio Git local.  
- Hacer commits de archivos en el repositorio.  
- Conectar con un repositorio en GitHub.  
- Enviar (*push*) y recibir (*pull*) archivos entre ambos.  

---

## 2. Configuraci√≥n inicial de Git

1. Abrir una terminal.  
2. Git ya est√° instalado, solo hay que configurarlo:  
   ```bash
   git config user.name "Tu Nombre"
   git config user.email "tu_email@ejemplo.com"
   ```
   - Esto permite que cada commit registre qui√©n lo realiz√≥.  
3. Verificar configuraci√≥n:  
   ```bash
   git config --list
   ```

---

## 3. Crear un repositorio local

1. En el explorador de archivos ‚Üí **Nuevo Folder** ‚Üí nombrarlo `demo`.  
2. En JupyterLab ‚Üí √≠cono de Git ‚Üí **Initialize Repository**.  
3. Ahora el directorio es un repositorio Git local.  

---

## 4. Autenticaci√≥n con GitHub mediante SSH

GitHub requiere autenticaci√≥n para hacer *push*. Esto se hace con un **par de claves SSH**.

1. Generar clave SSH:  
   ```bash
   ssh-keygen -t rsa -C "tu_email@ejemplo.com"
   ```
   - Se crean dos archivos:  
     - **Clave privada** (mantener en secreto).  
     - **Clave p√∫blica** (se puede compartir).  

2. Copiar la clave p√∫blica al portapapeles.  

3. Iniciar el agente SSH:  
   ```bash
   eval "$(ssh-agent -s)"
   ssh-add -k ~/.ssh/id_rsa
   ```

4. En GitHub ‚Üí **Settings ‚Üí SSH and GPG Keys ‚Üí New SSH Key** ‚Üí pegar la clave p√∫blica.  

---

## 5. Crear un repositorio en GitHub

1. En GitHub ‚Üí **New Repository** ‚Üí nombrarlo `demo`.  
2. Copiar la URL SSH (ejemplo: `git@github.com:usuario/demo.git`).  
3. En JupyterLab ‚Üí Git ‚Üí **Add Remote Repository** ‚Üí pegar la URL.  
4. Verificar conexi√≥n:  
   ```bash
   git remote -v
   ```

---

## 6. Flujo de trabajo: crear, trackear y commitear archivos

1. Crear un nuevo notebook (`File ‚Üí New ‚Üí Notebook`).  
2. Git lo mostrar√° como **Untracked**.  
3. Hacer clic derecho ‚Üí **Stage** para rastrearlo.  
4. Hacer commit:  
   - Mensaje: `"Initial Commit"`.  
   - Confirmar.  

---

## 7. Push al repositorio remoto

1. Configurar upstream (solo la primera vez):  
   ```bash
   git push --set-upstream origin master
   ```
2. Luego, simplemente:  
   ```bash
   git push
   ```

3. Verificar en GitHub ‚Üí el archivo y el mensaje de commit aparecen en el repo remoto.  

---

## 8. Ejemplo de actualizaci√≥n

1. Editar el notebook (ej. agregar c√°lculos).  
2. Guardar cambios.  
3. En la pesta√±a Git ‚Üí archivo aparece como **Changed**.  
4. Hacer **Stage** ‚Üí **Commit** (ej. mensaje `"some math"`).  
5. Hacer **Push to Remote**.  
6. Verificar en GitHub ‚Üí cambios reflejados.  

---

## 9. Resumen de la demo

En esta demostraci√≥n:

- Configuramos Git con nombre y correo.  
- Creamos un repositorio local.  
- Generamos un par de claves SSH y lo vinculamos a GitHub.  
- Creamos un repositorio en GitHub y lo enlazamos con el local.  
- Creamos un notebook, lo rastreamos, lo commiteamos y lo subimos a GitHub.  
- Finalmente, hicimos cambios, los volvimos a commitear y los sincronizamos con el remoto.  

---

üìå **Conclusi√≥n**:  
Este flujo demuestra c√≥mo **integrar Git y GitHub en un entorno de ciencia de datos con JupyterLab**, permitiendo versionar notebooks, colaborar y mantener sincronizados los repositorios locales y remotos.

---
# üîÑ M√≥dulo: Machine Learning Lifecycle  
## üìò Cap√≠tulo: ML Lifecycle Overview ‚Äì Parte 1

### 1. Introducci√≥n

Bienvenido al m√≥dulo sobre el ciclo de vida del aprendizaje autom√°tico (*Machine Learning Lifecycle*).  
Esta primera lecci√≥n ofrece una **visi√≥n general** del ciclo de vida de ML.  
Soy Wes Prichard, gerente principal de producto para Data Science y Servicios de IA en Oracle.

---

### 2. ¬øPor qu√© es importante el ciclo de vida de ML?

Las organizaciones buscan herramientas **vers√°tiles y productivas** para sus cient√≠ficos de datos, y desean que esas herramientas cubran **todo el ciclo de vida del aprendizaje autom√°tico**.  
Cuanto m√°s f√°cil y eficiente sea este ciclo, m√°s r√°pido y frecuentemente se podr√°n obtener resultados valiosos para la organizaci√≥n.

---

### 3. Las 6 etapas del ciclo de vida

Usaremos una versi√≥n simplificada del ciclo de vida, compuesta por **6 pasos**:

1. **Acceso a los datos**  
2. **Exploraci√≥n y preparaci√≥n de los datos**  
3. **Modelado** (construcci√≥n y entrenamiento del modelo)  
4. **Validaci√≥n del modelo**  
5. **Despliegue del modelo**  
6. **Monitoreo del modelo** (que puede llevar a su actualizaci√≥n o retiro)

üîπ Todo comienza con un **problema de negocio**, que define el objetivo del modelo.

---

### 4. Un proceso iterativo

Construir un modelo de ML es un proceso **iterativo**.  
Los pasos se repiten y ajustan hasta que el rendimiento del modelo sea satisfactorio.

üí° Existen representaciones m√°s complejas del ciclo de vida, como **CRISP-DM** (*Cross Industry Standard Process for Data Mining*), que pod√©s explorar por tu cuenta.  
Para este curso, usaremos el ciclo simplificado como marco para abordar las tareas clave del cient√≠fico de datos.

---

### 5. Acceso y adquisici√≥n de datos

Todo modelo de ML comienza con **datos**.  
En OCI Data Science, es √∫til almacenar los datos en la sesi√≥n de notebook para acceder r√°pidamente.

- El primer paso es **acceder y recopilar los datos** en el notebook.
- Conocer el **ecosistema de datos** de la organizaci√≥n ayuda a identificar fuentes potenciales.

#### Fuentes de datos comunes:
- **Sistemas de gesti√≥n de datos empresariales** (bases de datos, data lakes).
- **Pipelines de ingesti√≥n** desarrollados por ingenieros de datos y ML.
- **Datos no estructurados**: logs, texto, im√°genes, videos.
- **Cat√°logo de datos**: √∫til para localizar conjuntos existentes.
- **Fuentes externas**:
  - Datos p√∫blicos (gobiernos, open data).
  - Scraping web.
  - Proveedores de datos.
  - Encuestas, sensores, c√°maras, clickstream.

---

### 6. Exploraci√≥n y preparaci√≥n de datos

Una vez obtenidos los datos, el cient√≠fico de datos debe:

- **Explorar** y **visualizar** los datos.
- **Transformarlos** y repetir el proceso si es necesario.
- **Prepararlos**: limpieza y procesamiento antes del an√°lisis.

#### Tareas t√≠picas:
- Identificar y corregir datos corruptos, duplicados o incompletos.
- Determinar si los datos est√°n **etiquetados** (ej. im√°genes con bounding boxes).
- Si no lo est√°n, usar servicios como **OCI Data Labeling Cloud Service**.

---

### 7. An√°lisis exploratorio y estad√≠stico

Despu√©s de la limpieza, se analizan las **features** (variables):

- Identificar relaciones entre variables.
- Decidir transformaciones adicionales.
- Usar herramientas de an√°lisis estad√≠stico y visualizaci√≥n.

#### Preguntas clave:
- ¬øQu√© tipo de features hay?
- ¬øCu√°l es la distribuci√≥n de valores?
- ¬øHay valores nulos o inv√°lidos?
- ¬øExisten outliers?
- ¬øHay sesgos o correlaciones?
- ¬øEs necesario normalizar o transformar (ej. log)?
- ¬øC√≥mo manejar categor√≠as con cola larga?

---

### 8. Ingenier√≠a de features

Durante la exploraci√≥n, se pueden crear nuevas features que representen mejor los datos.

Ejemplo:  
En un dataset de tr√°fico con conteo por hora, pod√©s crear una feature que agrupe las horas en franjas como:

- Madrugada  
- Ma√±ana  
- Tarde  
- Noche  

Para features categ√≥ricas, suele ser necesario convertirlas en binarias (una por categor√≠a).

---

### 9. Modelado (inicio)

La etapa de modelado consiste en:

- Elegir el algoritmo de ML adecuado.
- Seleccionar las features que alimentar√°n el modelo.

üîπ En el primer paso, el cient√≠fico de datos debe decidir **qu√© tipo de modelo** es apropiado para resolver el problema planteado.

---

### 10. Tipos de aprendizaje autom√°tico

Existen dos tipos principales:

- **Aprendizaje supervisado**:  
  - El modelo aprende a partir de datos de entrada asociados a una **salida o etiqueta**.  
  - Ejemplos: clasificaci√≥n, regresi√≥n.

- **Aprendizaje no supervisado**:  
  - El modelo trabaja con datos **sin etiquetas**.  
  - Ejemplo: segmentaci√≥n de clientes ‚Üí el modelo asigna los segmentos autom√°ticamente.

---

### 11. Selecci√≥n de algoritmos y entrenamiento

- Se utilizan diferentes clases de modelos para problemas supervisados y no supervisados.
- Los cient√≠ficos de datos suelen probar **m√∫ltiples algoritmos** y generar **varios candidatos de modelos**.
- No se sabe de antemano cu√°l funcionar√° mejor ‚Üí se experimenta.

Durante el entrenamiento:

- Se prueban distintos **subconjuntos de features** como entrada.
- Reducir el n√∫mero de variables:
  - Disminuye el costo computacional.
  - Mejora la generalizaci√≥n.
  - Puede mejorar el rendimiento.

---

### 12. Divisi√≥n del conjunto de datos

- **Training set** ‚Üí para entrenar el modelo.  
- **Testing set** ‚Üí para evaluar el rendimiento en datos no vistos.

---

### 13. Evaluaci√≥n del modelo

Una vez entrenado, se debe evaluar su **idoneidad**.

- Hay herramientas open-source para calcular y visualizar m√©tricas.
- Elegir las m√©tricas adecuadas depende del **problema de negocio**.

#### Ejemplos:

- **Clasificaci√≥n**:
  - M√©trica com√∫n: **accuracy**.
  - Pero en casos como detecci√≥n de enfermedades raras, es mejor usar:
    - **Precisi√≥n** y **recall**.
    - **Matriz de confusi√≥n**: TP, TN, FP, FN.

- **Regresi√≥n**:
  - **RMSE** (error cuadr√°tico medio).
  - **MAE** (error absoluto medio).
  - **R¬≤** (coeficiente de determinaci√≥n).

- **No supervisado**:
  - Se busca que los **clusters** tengan alta cohesi√≥n interna.

---

### 14. Guardado de modelos

- Los modelos se guardan en formatos como:
  - **Pickle**
  - **ONNX**
  - **PMML**
- OCI Data Science ofrece un **cat√°logo de modelos** para preservarlos.

Seg√∫n el objetivo, el trabajo puede ser:

- Prueba de concepto.
- Experimentaci√≥n.
- Despliegue en producci√≥n.

---

### 15. Despliegue del modelo

Proceso de poner el modelo en uso.  
Tambi√©n se debe desplegar el **pipeline de transformaci√≥n de datos**.

- Los cient√≠ficos de datos colaboran con **ingenieros MLOps**.
- El despliegue puede ser:
  - **Batch**: inferencias programadas (ej. cada hora/d√≠a).
  - **Tiempo real**: activadas por eventos (ej. detecci√≥n de fraude en pagos).

#### Consideraciones:
- ¬øQu√© tan r√°pido se necesita la respuesta? ¬øMilisegundos o segundos?
- ¬øCu√°ntas solicitudes se esperan?
- ¬øQu√© tama√±o tienen los datos?

---

### 16. Monitoreo del modelo

Paso desafiante pero esencial para mantener la **eficacia** del modelo.

Dos componentes:

1. **Monitoreo estad√≠stico (drift)**:
   - Las m√©tricas pueden degradarse con el tiempo.
   - Ejemplo: valores fuera del rango del entrenamiento, cambios en la distribuci√≥n.

2. **Monitoreo operacional (ops)**:
   - Latencia de respuesta.
   - Uso de memoria y CPU.
   - Rendimiento y confiabilidad del sistema.
   - Logs y m√©tricas para diagn√≥stico.

---

### 17. Iteraci√≥n continua

El aprendizaje autom√°tico es un proceso **altamente iterativo**.  
Los pasos se repiten m√∫ltiples veces hasta alcanzar el objetivo de negocio.

---

### 18. Pr√≥ximas lecciones

En las siguientes lecciones, veremos c√≥mo **OCI Data Science** ayuda a los cient√≠ficos de datos a ejecutar cada etapa del ciclo de vida de ML.

---
# üì• Lecci√≥n: Access Data ‚Äì üîç Acceso a datos en OCI Data Science

## 1. Introducci√≥n

Hola y bienvenido a la siguiente lecci√≥n del curso de Oracle Cloud Infrastructure Data Science.  
Soy Himanshu Raj, cient√≠fico de datos y l√≠der senior de entrenamiento en AI/ML en Oracle.

En esta lecci√≥n aprenderemos sobre el **primer paso del ciclo de vida del aprendizaje autom√°tico**:  
üëâ **Acceder a los datos**.

Tambi√©n veremos:

- Por qu√© necesitamos datos.
- C√≥mo se recopilan.
- Cu√°les son las fuentes clave para acceder a datos en OCI Data Science.

---

### 2. ¬øPor qu√© necesitamos datos?

Toda aplicaci√≥n o servicio, digital o no, **genera informaci√≥n**.  
Esta informaci√≥n puede clasificarse seg√∫n su tama√±o o fuente:

- **Datos por lotes (batch)**: generados con el tiempo por cargas diarias (ej. backups, migraciones).
- **Datos de servicios de streaming**: mensajes o logs de eventos de usuario e IoT.
- **Datos de aplicaciones**: generados por llamadas a APIs, eventos de aplicaciones, archivos de log, etc.

üîÅ Estos datos deben ser **tra√≠dos a OCI** para su preprocesamiento y entrenamiento de modelos.  
Pod√©s acceder a ellos desde la **interfaz gr√°fica** o desde la **l√≠nea de comandos** usando librer√≠as espec√≠ficas.

---

### 3. ¬øQu√© rol cumple el dato en ciencia de datos?

La ciencia de datos es una disciplina multidisciplinaria que necesita datos para:

- Formular hip√≥tesis y extraer conclusiones.
- Realizar investigaciones basadas en datos.
- Resolver problemas concretos.

üí° Preguntas clave:
- ¬øQu√© tipo de datos necesito para resolver este problema?
- ¬øCon los datos que ya tengo, puedo resolver problemas existentes?

---

### 4. Fuentes clave de datos en OCI Data Science

Estas son algunas de las fuentes m√°s comunes (aunque no las √∫nicas):

- **OCI Object Storage**
- **Almacenamiento local**
- **Oracle Autonomous Databases**
- **MySQL**
- **Amazon S3**
- **Endpoints HTTPS**
- **DatasetBrowser**
- **PyArrow**

---

### 5. Acceso a Oracle Object Storage

Para cargar un `DataFrame` desde Object Storage:

- Us√° el ejemplo proporcionado, reemplazando el nombre del bucket y archivo.
- Pod√©s autenticarte usando:
  - **API Key**
  - **Resource Principal** (usualmente en funciones serverless)

üîê El m√≥dulo `set_auth` permite habilitar o deshabilitar la identidad del principal o del par de claves en una sesi√≥n abierta.

üìö Para m√°s detalles, consult√° la documentaci√≥n de la clase ADS.

---

### 6. Acceso a almacenamiento local

Pod√©s acceder a archivos locales usando funciones como:

```python
pandas.read_csv("ruta/al/archivo.csv")
```

---

### 7. Acceso a Oracle Autonomous Databases

OCI Data Science soporta ambos servicios de Autonomous Database.

- Us√° `ads.read_sql`, que es **15 veces m√°s r√°pido** que `pandas.read_sql`.
- Esto se debe a que **evita el ORM** y est√° optimizado para bases de datos Oracle.

#### Si us√°s un wallet file:
- Defin√≠ los par√°metros de conexi√≥n y la ubicaci√≥n del wallet.
- Luego ejecut√° la consulta con `ads.read_sql`.

#### Si no us√°s wallet:
- Defin√≠ `hostname` y `port` en el diccionario `connection_parameters`.
- ‚ö†Ô∏è Esta opci√≥n est√° disponible solo en **ADS versi√≥n 2.5.6 o superior**.

üîê Se recomienda usar **bind variables** para evitar ataques de inyecci√≥n SQL.

üìâ El rendimiento puede verse afectado por factores como la red, latencia, etc.

---
# üì• Lecci√≥n: Access Data ‚Äì Parte 2  
## üîç Acceso a datos en OCI Data Science (continuaci√≥n)

### 8. Optimizaci√≥n del acceso a bases de datos

El **tiempo de respuesta** de una base de datos puede mejorar significativamente mediante:

- Uso de **√≠ndices**.
- Escritura de **consultas SQL eficientes**.

üîπ Aunque la red de OCI es muy r√°pida, factores como VPNs o topolog√≠as complejas pueden afectar el rendimiento.  
Es importante considerar el **tiempo necesario para acceder a los datos**.

---

### 9. Acceso a MySQL

Pod√©s seguir los mismos pasos que con Oracle Autonomous Database, pero:

- Deb√©s definir el motor como `"MySQL"`.
- Esta funcionalidad est√° disponible a partir de **ADS versi√≥n 2.5.6**.

Para guardar un `DataFrame` en MySQL:

```python
ads.to_sql(df, engine="MySQL", ...)
```

---

### 10. Acceso a Amazon S3

- Archivos p√∫blicos o privados de **Amazon S3** pueden ser accedidos v√≠a `pandas`.
- Para archivos privados, deb√©s pasar las **credenciales correctas** usando el diccionario `storage_options` de ADS.

---

### 11. Acceso v√≠a HTTP/HTTPS

Tambi√©n pod√©s acceder a datos desde **URLs** usando `pandas`:

```python
pd.read_csv("https://ejemplo.com/datos.csv")
```

---

### 12. Uso de DatasetBrowser

ADS incluye el m√©todo `DatasetBrowser` para acceder f√°cilmente a conjuntos de datos bien definidos desde bibliotecas de referencia como:

- **Seaborn**
- **Scikit-learn**
- **GitHub**

Pod√©s listar los datasets disponibles con:

```python
DatasetBrowser.list()
```

Y abrir uno espec√≠fico con:

```python
DatasetBrowser.open("nombre_dataset")
```

---

### 13. Acceso a datos con PyArrow y OCI FS

ADS tambi√©n permite editar y procesar datos grandes usando **PyArrow** a trav√©s de **OCI File Systems (OCI FS)**.

- OCI FS es un sistema de archivos Pythonic que:
  - Contiene informaci√≥n de conexi√≥n.
  - Permite operaciones t√≠picas de sistema de archivos.

---

### 14. Detecci√≥n de tipos sem√°nticos de datos

ADS detecta autom√°ticamente los **tipos sem√°nticos** al abrir un dataset:

- **Categ√≥ricos**: ej. color de ojos, talla de camisa.
- **Ordinales**: ej. nivel educativo (primaria, secundaria, universidad).
- **Continuos**: ej. altura, versiones de software.
- **Fechas y horas**: formato datetime.

Pod√©s inspeccionar los tipos con:

```python
df.feature_types
df.show_in_notebook()
```

---

### 15. Fuentes y formatos soportados por ADS

ADS soporta m√∫ltiples fuentes y formatos de datos en OCI Data Science.  
üìö Est√°n listados en la [documentaci√≥n oficial](https://docs.oracle.com/es-ww/iaas/Content/data-science/using/overview.htm)¬π.

üîπ No se soportan directamente:
- Archivos `.txt`, `.doc`, `.pdf`
- Im√°genes sin procesar
- Estructuras como `list`, `tuple`, `range`

Pero ADS incluye un **m√≥dulo de extracci√≥n de texto** para convertir `.PDF` o `.DOC` en texto plano.

---

### 16. Cierre de la lecci√≥n

Esperamos que esta lecci√≥n te haya sido √∫til para aprender c√≥mo acceder a datos desde fuentes comunes en Oracle Data Science.  
Este paso es esencial para iniciar cualquier flujo de trabajo de machine learning.

---

---

# üîç Lecci√≥n: Data Exploration and Preparation  
## üìò Exploraci√≥n y preparaci√≥n de datos en OCI Data Science

### 1. Introducci√≥n

Hola y bienvenido a esta nueva lecci√≥n del curso de Oracle Cloud Infrastructure Data Science.  
Soy Himanshu Raj, cient√≠fico de datos y l√≠der de entrenamiento en AI/ML en Oracle.

En esta lecci√≥n abordaremos el **segundo paso del ciclo de vida del aprendizaje autom√°tico**:  
üëâ **Exploraci√≥n y preparaci√≥n de datos**.

Veremos:

- Por qu√© es necesario el preprocesamiento.
- Qu√© pasos incluye.
- Herramientas de transformaci√≥n de ADS.
- C√≥mo dividir los datos en conjuntos de entrenamiento, prueba y validaci√≥n.

---

### 2. ¬øPor qu√© preprocesar los datos?

Los datos reales suelen tener:

- Valores faltantes.
- Errores.
- Outliers.
- Formatos inconsistentes.

üîß Por eso, antes de buscar patrones, debemos **limpiar y transformar** los datos.

El preprocesamiento puede incluir varios pasos, seg√∫n el problema y el tipo de datos.  
üí° Es com√∫n que esta etapa sea la m√°s extensa del ciclo de vida de ML.

---

### 3. Operaciones b√°sicas sobre datos

Cuando los datos provienen de m√∫ltiples fuentes, debemos **combinarlos**.  
ADS permite realizar operaciones como:

- Agregar o eliminar filas/columnas.
- Filtrar.
- Concatenar vertical u horizontalmente.
- Unir por columnas o √≠ndices.

üìå Las operaciones de `pandas` tambi√©n se aplican a objetos `ADSData`.

---

### 4. Limpieza y validaci√≥n

Es importante verificar:

- Formatos y unidades.
- Convenciones de nombres.
- Tipos de datos.
- Valores nulos.
- Duplicados.
- Estad√≠sticas descriptivas.

---

### 5. Imputaci√≥n de valores faltantes

Los valores faltantes pueden deberse a errores humanos o t√©cnicos.  
Ejemplo: coordenadas GPS incorrectas por mal clima.

Opciones:

- ‚ùå Eliminar filas incompletas (no recomendado).
- ‚úÖ Imputar con:
  - Media o mediana (para datos num√©ricos).
  - Moda (para datos categ√≥ricos).

---

### 6. Codificaci√≥n de variables categ√≥ricas

- **Label Encoding** (`label_encoder`): convierte categor√≠as en n√∫meros.  
  ‚ö†Ô∏è No recomendable para datos ordinales.

- **One Hot Encoding**:
  - Convierte una columna categ√≥rica en varias columnas binarias.
  - Se puede hacer con `pandas.get_dummies()` o `fit_transform()`.

---

### 7. Detecci√≥n de outliers

Los outliers pueden ser errores o datos v√°lidos pero at√≠picos.

- Se detectan con:
  - Visualizaciones: scatterplot, boxplot.
  - Estad√≠sticas: desviaci√≥n est√°ndar, distribuci√≥n gaussiana.
  - Algoritmos de ML (supervisado o no supervisado).

üìå En aprendizaje no supervisado, se asume que los outliers son pocos y no siguen la misma tendencia.

---

### 8. Escalado de caracter√≠sticas

El escalado ajusta las variables a una misma escala.  
Es √∫til en algoritmos sensibles a distancias (ej. regresi√≥n).

- **Normalizaci√≥n (Min-Max)**: valores entre 0 y 1.
- **Estandarizaci√≥n**: media 0, desviaci√≥n est√°ndar 1 ‚Üí distribuci√≥n normal.

---

### 9. Reducci√≥n de dimensionalidad

La **dimensionalidad** es el n√∫mero de variables de entrada.

- Alta dimensionalidad = mayor costo computacional.
- Dos enfoques:
  - **Selecci√≥n de caracter√≠sticas**: elegir un subconjunto.
  - **Extracci√≥n de caracter√≠sticas**: crear nuevas variables a partir de las existentes.

---

### 10. Preprocesamiento de texto

Para datos textuales, se aplican t√©cnicas como:

- Vectorizaci√≥n.
- Eliminaci√≥n de stop words.
- Tokenizaci√≥n.
- POS tagging.
- Stemming y lematizaci√≥n.

---

### 11. Herramientas de transformaci√≥n en ADS

#### a. `suggest_recommendations`

- Detecta problemas en el dataset.
- Sugiere transformaciones.
- Pod√©s aceptar los cambios desde el men√∫.
- Luego se obtiene el dataset transformado con `get_transformed_dataset()`.

#### b. `auto_transform`

- Aplica todas las recomendaciones autom√°ticamente.
- Imputa valores faltantes.
- Elimina columnas altamente correlacionadas.
- Maneja clases desbalanceadas (upsampling/downsampling).
- Elimina claves primarias y columnas sin valor predictivo.

#### c. `visualize_transforms`

- Muestra visualmente las transformaciones aplicadas.
- Solo funciona con transformaciones autom√°ticas.

---

### 12. Ejemplo pr√°ctico

En un dataset de rotaci√≥n de empleados:

- ADS detecta tipos de datos.
- Sugiere transformaciones.
- Muestra correlaciones fuertes y desbalance de clases.
- Visualiza el flujo de transformaci√≥n.

üìå Los resultados var√≠an seg√∫n el dataset.

---

### 13. Divisi√≥n de datos

Dividir el dataset en:

- **Entrenamiento**
- **Prueba**
- **Validaci√≥n**

Permite evaluar la **generalizaci√≥n** del modelo.

Por defecto, ADS usa:

- 80% entrenamiento
- 10% prueba
- 10% validaci√≥n

üí° En datasets peque√±os, puede ser mejor usar 70% o 60% para entrenamiento.

---

### 14. Conclusi√≥n

Esta lecci√≥n cubri√≥:

- Preprocesamiento de datos reales.
- Herramientas de transformaci√≥n en ADS.
- Codificaci√≥n, imputaci√≥n, escalado y detecci√≥n de outliers.
- Divisi√≥n en conjuntos de entrenamiento, prueba y validaci√≥n.

---
# üß™ Lecci√≥n: Demo de Preprocesamiento con ADS  
## üìò Ejemplo pr√°ctico en OCI Data Science

### 1. Introducci√≥n

En esta demo realizaremos un ejercicio pr√°ctico de preprocesamiento de datos usando **Accelerated Data Science (ADS)** en una sesi√≥n de notebook de OCI Data Science.

Usaremos el dataset **Employee Attrition**, que contiene:

- **1.470 filas**
- **36 caracter√≠sticas**:
  - 22 ordinales
  - 11 categ√≥ricas
  - 3 constantes

Las variables incluyen informaci√≥n demogr√°fica, nivel de compensaci√≥n, caracter√≠sticas del puesto, satisfacci√≥n laboral y m√©tricas de desempe√±o.  
üìâ El dataset est√° **desbalanceado**: hay menos empleados que se van que los que se quedan.

---

### 2. Carga del dataset

1. Se importan las librer√≠as necesarias, incluyendo ADS.
2. Se carga el `DataFrame` desde **Object Storage** usando el m√©todo de **resource principal**.
3. Se define el bucket, el namespace y se usa `DatasetFactory.open()` para acceder al dataset.
4. Se establece la **feature objetivo**: `attrition`.

---

### 3. Transformaciones sugeridas

#### a. `suggest_recommendations`

- Detecta problemas en el dataset.
- Muestra mensajes con variables afectadas, acciones sugeridas y c√≥digo para aplicarlas.

#### b. `auto_transform`

- Aplica autom√°ticamente todas las transformaciones recomendadas.
- Optimiza el dataset:
  - Imputa valores faltantes.
  - Elimina columnas altamente correlacionadas.
  - Maneja clases desbalanceadas.
  - Elimina claves primarias y columnas sin valor predictivo.

#### c. `visualize_transforms`

- Permite visualizar las transformaciones aplicadas.
- Muestra diferencias entre aplicar o no `auto_transform`.

---

### 4. Codificaci√≥n de variables categ√≥ricas

Ejemplo con la variable `job_function`:

- Se observan tres categor√≠as distintas.
- Se usa el codificador categ√≥rico de ADS:
  ```python
  from ads.dataset.labelencoder import LabelEncoder
  ```
- Las categor√≠as se transforman en valores num√©ricos.

---

### 5. Balanceo de clases

- Se realiza **upsampling** usando:
  ```python
  from ads.dataset.helper import upsample
  ```
- Se observa la repetici√≥n de muestras en la clase minoritaria (`attrition = yes`).
- Se comparan los conteos antes y despu√©s del balanceo.

---

### 6. Divisi√≥n del dataset

Una vez completadas las transformaciones:

- Se divide el dataset en:
  - **Entrenamiento** (80%)
  - **Prueba** (10%)
  - **Validaci√≥n** (10%)

üìå Estos valores pueden ajustarse seg√∫n el tama√±o del dataset.

---

### 7. Recursos adicionales

- Documentaci√≥n oficial de ADS  
- Laboratorios en GitHub: [oracle-samples/oci-data-science-ai-samples](https://github.com/oracle-samples/oci-data-science-ai-samples)¬π  
- Ejemplos de notebooks en la sesi√≥n de Data Science

---


# üìä Lecci√≥n: Data Visualization and Profiling  
## üìò Visualizaci√≥n de datos y perfilado en OCI Data Science

### 1. Introducci√≥n

Hola, soy Jon Stanesby. En esta lecci√≥n aprenderemos sobre **visualizaci√≥n de datos** y **perfilado**.

La visualizaci√≥n de datos es una parte esencial de la **exploraci√≥n y an√°lisis de datos** en ciencia de datos moderna.  
Es uno de los primeros pasos para **extraer valor** de los datos, ya que permite identificar patrones y relaciones de forma r√°pida y accesible, incluso sin formaci√≥n t√©cnica especializada.

---

### 2. ¬øQu√© es Data Visualization (DV)?

DV es la **presentaci√≥n gr√°fica de los datos** para ilustrar hallazgos y explicar resultados.  
Es clave para:

- Analizar datos.
- Tomar decisiones basadas en datos.
- Comunicar patrones y relaciones de forma clara.

---

### 3. Herramientas de visualizaci√≥n modernas

Las herramientas de DV suelen:

- Conectarse con fuentes de datos (bases relacionales, cloud, on-premise).
- Ofrecer m√∫ltiples opciones de visualizaci√≥n.
- Sugerir autom√°ticamente el tipo de gr√°fico seg√∫n los datos.
- Integrar IA/ML para facilitar tareas a usuarios no t√©cnicos.
- Permitir acceso y colaboraci√≥n en toda la organizaci√≥n.
- Ofrecer flexibilidad entre control manual y automatizaci√≥n.

üîπ Busc√° herramientas con:
- Interfaz intuitiva (point & click, drag & drop).
- Capacidad de edici√≥n r√°pida.
- Visualizaci√≥n autom√°tica de datos.

---

### 4. Visualizaci√≥n inteligente con ADS

ADS ofrece herramientas de visualizaci√≥n **autom√°ticas y personalizables**:

- Detecta autom√°ticamente el tipo de datos.
- Genera gr√°ficos √≥ptimos para cada variable.
- Permite usar cualquier librer√≠a de visualizaci√≥n (Seaborn, Matplotlib, etc.).

#### Visualizaciones comunes en ADS:
- Estad√≠sticas resumen.
- Gr√°ficos de distribuci√≥n.
- Mapas de correlaci√≥n.
- Detecci√≥n de anomal√≠as (valores faltantes, alta cardinalidad).

---

### 5. M√©todos autom√°ticos en ADS

#### a. `corr` (correlaci√≥n)
- Calcula matrices de correlaci√≥n por tipo de variable:
  - **Continua-Continua**: coeficiente de Pearson (‚àí1 a 1).
  - **Continua-Categ√≥rica**: ratio de correlaci√≥n (0 a 1).
  - **Categ√≥rica-Categ√≥rica**: Cramer's V (0 a 1).

#### b. `show_in_notebook`
- Muestra una vista completa del dataset:
  - Tipo de problema (regresi√≥n, clasificaci√≥n binaria o multiclase).
  - N√∫mero de filas y columnas.
  - Tipos de features.
  - Visualizaciones por columna.
  - Mapa de correlaci√≥n.
  - Encabezado del dataset.

üìå Usa una muestra inteligente con 95% de confianza y 1% de intervalo.

#### c. `plot`
- Herramienta autom√°tica de gr√°ficos.
- Se define `x` (y opcionalmente `y`).
- ADS elige el tipo de gr√°fico seg√∫n los tipos de datos.

Ejemplos:

- `x = attrition` (categ√≥rica binaria) ‚Üí gr√°fico de barras.
- `x = edad` (continua) ‚Üí histograma.
- `x = categ√≥rica`, `y = continua` ‚Üí violin plot.
- `x = continua`, `y = continua` ‚Üí heatmap gaussiano + scatterplot.

---

### 6. Sistema de tipos de features

ADS extiende los `DataFrames` de Pandas para:

- Separar representaci√≥n f√≠sica vs significado de los datos.
- Almacenar propiedades y m√©todos por feature.
- Usar herencia m√∫ltiple para definir caracter√≠sticas.
- Validar y advertir sobre calidad de datos.

üìå `feature_plot`:
- Sobre una serie ‚Üí gr√°fico univariado.
- Sobre un `DataFrame` ‚Üí tabla con visualizaci√≥n por feature.

---

### 7. Visualizaci√≥n personalizada

ADS permite usar librer√≠as externas:

#### a. `call` method
- Permite definir tu propia rutina de visualizaci√≥n.

Ejemplos:

- `Seaborn.pairplot(df)` ‚Üí relaciones entre pares de features.
- `Matplotlib` ‚Üí gr√°ficos personalizados.
- Dataset de terremotos en California ‚Üí visualizaci√≥n geogr√°fica.

---

### 8. Conclusi√≥n

En esta lecci√≥n aprendiste a:

- Generar visualizaciones autom√°ticas con ADS.
- Personalizar gr√°ficos seg√∫n tus necesidades.
- Usar m√©todos como `corr`, `plot`, `feature_plot`, `show_in_notebook`.
- Integrar librer√≠as externas para visualizaci√≥n avanzada.

---

# üß† Lecci√≥n: Model Training  
## üìò Entrenamiento de modelos en OCI Data Science

### 1. Introducci√≥n

Hola, soy Jon Stanesby. En esta lecci√≥n aprenderemos sobre el **entrenamiento de modelos**, una etapa cr√≠tica dentro de la fase de modelado del ciclo de vida del aprendizaje autom√°tico.

---

### 2. ¬øQu√© es el entrenamiento de modelos?

El entrenamiento de modelos construye la **mejor representaci√≥n matem√°tica** de las relaciones entre:

- Las **features** y la **etiqueta objetivo** (en aprendizaje supervisado).
- Todas las **features** (en aprendizaje no supervisado).

üîπ El proceso genera un **artefacto** que captura estos patrones.  
üîπ Se selecciona el **mejor algoritmo** considerando m√∫ltiples dimensiones.

---

### 3. Componentes del proceso de entrenamiento

#### a. Funci√≥n de puntuaci√≥n (`score function`)
- Indica qu√© tan bien se ajusta el modelo.
- Puede ser una funci√≥n de error o de m√°xima verosimilitud.

#### b. Funci√≥n de p√©rdida (`loss function`)
- Compara las predicciones del modelo con los valores reales.
- Calcula una **puntuaci√≥n de p√©rdida** como n√∫mero √∫nico.

üìä Ejemplo gr√°fico:
- Puntos verdes ‚Üí datos reales.
- L√≠nea negra ‚Üí predicciones.
- Flechas rojas ‚Üí error (p√©rdida).

#### c. Funci√≥n de actualizaci√≥n (`update function`)
- Ajusta los par√°metros del modelo en cada iteraci√≥n.

---

### 4. Frameworks y entornos de entrenamiento

En ciencia de datos, **open source** se refiere a c√≥digo libre y modificable.  
Los frameworks open source:

- Son accesibles.
- Tienen comunidades activas.
- Fomentan la innovaci√≥n y soluci√≥n de bugs.

üîπ OCI Data Science combina frameworks **propietarios de Oracle** y **open source**.  
üîπ Pod√©s instalar librer√≠as externas desde la terminal o iniciar con tu propio set de herramientas.

---

### 5. Formas de entrenar modelos en OCI

Pod√©s entrenar modelos de varias maneras:

- üß™ **Notebooks**: escribiendo y ejecutando c√≥digo Python en JupyterLab.
- ‚öôÔ∏è **Entornos Conda**: usando ADS, MLX o AutoML (veremos m√°s adelante).
- üßµ **Jobs**: se cubren en el m√≥dulo 4.

---

### 6. Conclusi√≥n

En esta lecci√≥n vimos:

- Qu√© es el entrenamiento de modelos.
- C√≥mo se representa matem√°ticamente la relaci√≥n entre variables.
- Qu√© funciones intervienen en el proceso.
- Qu√© frameworks y entornos se pueden usar.
- Qu√© opciones ofrece OCI para entrenar modelos.

---

# üöÄ Lecci√≥n: Turning AML Models on OCI  
## üìò Entrenamiento y escalado de modelos AML en Oracle Cloud

### 1. Introducci√≥n

¬°Felicitaciones por llegar tan lejos en el curso de ciencia de datos!  
Soy Himanshu Raj, l√≠der senior de entrenamiento en AI/ML en Oracle.

En este video experto hablaremos sobre c√≥mo **entrenar y escalar modelos de aprendizaje autom√°tico (AML)** en Oracle Cloud Infrastructure (OCI).

---

### 2. Entrenamiento b√°sico de modelos AML

Pod√©s entrenar f√°cilmente un modelo AML usando **jobs** del servicio de ciencia de datos.

üîπ ¬øQu√© necesit√°s?

- C√≥digo fuente alojado en **GitHub**.
- Resultados almacenados en **OCI Object Storage**.
- Definir recursos y ejecuci√≥n con **ADS**:
  - Usando c√≥digo Python.
  - O mediante archivos **YAML**.

---

### 3. Entrenamiento distribuido en OCI

Para escalar horizontalmente y paralelizar tareas de entrenamiento en datasets grandes o cargas intensivas:

‚úÖ OCI Data Science permite **entrenamiento distribuido** con ayuda de ADS.

üîπ Frameworks soportados:

- **Dask**
- **PyTorch Distributed**
- **Horovod**
- **TensorFlow Distributed**

üìå Esto permite reducir tiempos de entrenamiento sin perder precisi√≥n.

---

### 4. Implementaci√≥n y comunidad

- Pod√©s usar **Docker** o **GitHub** para tus implementaciones.
- La documentaci√≥n oficial detalla c√≥mo configurar cada framework.
- Se recomienda compartir tus casos de uso en la comunidad **OU**.

---

### 5. AutoMLx en OCI

Tambi√©n cubrimos **AutoML** en el curso.

üîπ Te recomendamos explorar el paquete **AutoMLx**, disponible en el **conda pack** de OCI.

- AutoMLx proporciona un pipeline que:
  - Encuentra y ajusta autom√°ticamente el mejor modelo.
  - A partir de una tarea de predicci√≥n y un dataset de entrenamiento.

Pod√©s elegir el motor paralelo (`task` o `local`) usando la funci√≥n `INIT`.

---

### 6. Recursos y seguimiento

- Consult√° la documentaci√≥n de **ADS** y de las clases alias.
- Revis√° las **release notes** para estar al d√≠a con nuevas funcionalidades.
- Compart√≠ tus avances y dudas en la comunidad **OU**.

---

### 7. Conclusi√≥n

En esta lecci√≥n aprendiste:

- C√≥mo entrenar modelos AML en OCI usando jobs.
- C√≥mo escalar horizontalmente con entrenamiento distribuido.
- C√≥mo usar AutoMLx para automatizar el ajuste de modelos.
- D√≥nde encontrar documentaci√≥n y c√≥mo participar en la comunidad.

---



</br>
</br>
</br>
</br>
</br>































***********************************************************************************************************************************************


---

### üìò Glosario de T√©rminos Clave en OCI Data Science

| M√≥dulo / √Årea         | T√©rmino clave            | Descripci√≥n breve                                                  | Ejemplo aplicado a hotel real                                     |
|-----------------------|--------------------------|--------------------------------------------------------------------|--------------------------------------------------------------------|
| Entorno de desarrollo | ADS SDK                  | Librer√≠a de Python que simplifica tareas en el ciclo ML            | Entrenar modelo para predecir cancelaciones                       |
| Entorno de desarrollo | Notebook Session         | Ambiente interactivo en OCI para ejecutar c√≥digo y analizar datos  | Editar notebook para detecci√≥n de anomal√≠as en reservas            |
| IAM                   | Tenancy                  | Entorno ra√≠z que agrupa todos los recursos en OCI                  | Tenencia ‚ÄúHotelCorp‚Äù con compartimientos separados                |
| IAM                   | Compartment              | Contenedor l√≥gico para organizar recursos y pol√≠ticas              | Compartimiento ‚ÄúMarketing‚Äù con notebooks y objetos                |
| IAM                   | Group / Dynamic Group    | Grupo de usuarios o recursos con pol√≠ticas asociadas               | Grupo ‚ÄúAnalistas‚Äù con acceso a m√©tricas y logs                    |
| IAM                   | User / Principal         | Identidad que puede autenticarse y operar sobre recursos           | Usuario ‚ÄúAnaRecepcion‚Äù con acceso a modelos y buckets              |
| IAM                   | Policy / IAM Settings    | Reglas que autorizan acciones sobre recursos                       | Pol√≠tica que permite acceso al Object Storage                     |
| IAM                   | OCI CLI / Token          | Herramientas para operar OCI desde la terminal                     | Usar CLI para lanzar notebook desde cron semanal                  |
| Automatizaci√≥n        | Job / Pipeline / Workload| Ejecuciones automatizadas en infraestructura OCI                   | Job nocturno que actualiza predicci√≥n de ocupaci√≥n                |
| Automatizaci√≥n        | Scheduler / Cron         | Programador de tareas para ejecuci√≥n peri√≥dica                     | Ejecutar script todos los d√≠as a las 6 AM                         |
| Automatizaci√≥n        | Logging / Audit          | Registro de actividad y monitoreo del sistema                      | Ver qui√©n modific√≥ modelo de predicci√≥n ayer                      |
| Ciencia de datos      | ML Lifecycle             | Ciclo completo: ingesti√≥n, entrenamiento, despliegue, monitoreo    | Desde historial de reservas hasta API en producci√≥n               |
| Ciencia de datos      | AutoML / ADSTuner        | Entrenamiento autom√°tico y ajuste de hiperpar√°metros               | Entrenar 20 modelos y elegir el m√°s preciso                       |
| Ciencia de datos      | Model Catalog / Deployment| Repositorio y endpoints HTTP para modelos ML                       | Compartir modelo con el equipo de recepci√≥n                       |
| Ciencia de datos      | Feature Engineering      | Transformar variables para mejorar modelos                         | Crear ‚Äútemporada alta‚Äù desde campo fecha                          |
| Ciencia de datos      | Train-Test Split         | Divisi√≥n de datos para entrenamiento y evaluaci√≥n                  | Usar datos de 2021 para entrenar y 2022 para testear              |
| Ciencia de datos      | Metrics / Evaluation     | Indicadores para medir la calidad de un modelo                     | Ver precisi√≥n y F1 Score del modelo de ocupaci√≥n                  |
| Visualizaci√≥n         | Feature Types / Heatmap    | Clasificaci√≥n de variables y visualizaci√≥n de relaciones            | Ver correlaci√≥n entre ocupaci√≥n y d√≠a de la semana                  |
| Visualizaci√≥n         | Histogram / Distribution   | Gr√°fico que muestra frecuencia de valores                           | Distribuci√≥n de estad√≠as por cantidad de noches                    |
| Visualizaci√≥n         | Boxplot / Outliers         | Visualizaci√≥n de rango y valores extremos                           | Detectar precios fuera de lo com√∫n en reservas                     |
| Visualizaci√≥n         | Pairplot / Correlation     | Comparaci√≥n cruzada entre variables num√©ricas                       | Relaci√≥n entre tarifa, ocupaci√≥n y tipo de habitaci√≥n              |
| Almacenamiento        | Object Storage / Bucket    | Almacenamiento escalable en OCI para datasets y resultados          | Guardar CSV con reservas hist√≥ricas                                |
| Almacenamiento        | Dataset / CSV / JSON       | Formatos de datos para an√°lisis y entrenamiento de modelos          | Subir archivo ‚Äúcancelaciones_2023.csv‚Äù al bucket                   |
| Almacenamiento        | Data Flow / Data Catalog   | Servicios para procesar y documentar grandes vol√∫menes de datos     | Catalogar datasets sobre temporada alta y baja                     |
| Seguridad             | Vault / Secret Keeper      | Gesti√≥n segura de credenciales y claves                             | Conectar notebook con base Oracle sin exponer claves               |
| Seguridad             | Encryption / Access Policy | Protecci√≥n de datos y reglas de acceso                              | Definir pol√≠tica que proh√≠ba modificar modelos desde marketing     |
| Redes                 | VCN / Subnet / NAT / SG    | Red virtual y componentes para tr√°fico interno y externo            | Acceder a Object Storage sin exponer IP p√∫blica                    |
| Redes                 | Public / Private Endpoint  | Configuraci√≥n de acceso a servicios desde dentro o fuera de OCI     | API de predicci√≥n solo accesible desde subnet del hotel            |
| Redes                 | Internet Gateway / Route   | Permisos para que una red tenga salida a internet                   | Notebook con acceso a repositorios externos                        |
| Pr√°cticas MLOps       | Escalado / Load Balancer   | Optimizaci√≥n de recursos para alta demanda                          | Aumentar recursos si el uso del modelo crece durante temporada alta|
| Pr√°cticas MLOps       | Monitoring / Logging       | Seguimiento de m√©tricas y eventos en producci√≥n                     | Ver cu√°ntas veces se consult√≥ el modelo desde recepci√≥n            |
| Pr√°cticas MLOps       | Explainability / Feature Importance| Herramientas para interpretar el modelo                     | Mostrar que ‚Äútipo de cliente‚Äù impacta m√°s en cancelaciones         |
| Pr√°cticas MLOps       | Retraining / Drift         | Reentrenamiento y detecci√≥n de cambios en los datos                 | Actualizar modelo si cambian patrones de reserva                   |
| SDK / Herramientas    | Python SDK / OCI SDK       | Librer√≠as para interactuar con servicios OCI desde c√≥digo           | Crear bucket y notebook usando Python desde script                    |
| SDK / Herramientas    | ADS SDK / ADSInterpreter   | Herramientas espec√≠ficas para ciencia de datos en OCI               | Generar explicaci√≥n de modelo con gr√°ficos autom√°ticos                |
| SDK / Herramientas    | OCI CLI                    | Interfaz de l√≠nea de comandos para operar servicios OCI             | Lanzar notebook o subir archivos directamente desde consola           |
| Infraestructura       | Terraform / Resource Manager| Infraestructura como c√≥digo para automatizar configuraci√≥n          | Crear compartimientos y pol√≠ticas de acceso de forma repetible        |
| Infraestructura       | Stack / Plan / Apply       | Proceso de despliegue con Terraform: definir, previsualizar, ejecutar| Desplegar recursos para nuevo hotel sin errores manuales              |
| Organizaci√≥n           | Tags / Cost Tracking        | Etiquetado de recursos y seguimiento de costos                      | Ver cu√°nto cuesta entrenar modelos en entorno ‚ÄúTemporadaAlta‚Äù         |
| Buenas pr√°cticas      | Modularizaci√≥n / Reusabilidad| Separar funciones para claridad y mantenimiento                     | Validaciones de reserva en m√≥dulo ‚Äúutils‚Äù                             |
| Buenas pr√°cticas      | Confirmaci√≥n / Validaci√≥n  | Evitar errores cr√≠ticos con chequeos y confirmaciones expl√≠citas    | Confirmar borrado de modelo con ‚Äú¬øEst√°s seguro? [y/n]‚Äù                |
| Buenas pr√°cticas      | Documentaci√≥n / Naming     | Uso de nombres claros y descripciones en c√≥digo y estructura        | Bucket ‚Äúdatos_cancelaciones_hotelA_2023‚Äù                              |
| Bonus Ctrl+BA         | Ficha t√©cnica / Glosario Ctrl| Recurso con t√©rminos clave y ejemplos integrados                    | Usar en mentor√≠as dentro del servidor Ctrl+BA                         |
| Bonus Ctrl+BA         | Cuestionarios / Flashcards | Material para repasar y autoevaluarse en 3 niveles                  | Fichas para revisar IAM, ML y MLOps antes del examen                  |
| Bonus Ctrl+BA         | Presentaci√≥n / Teaching Pack| Recurso did√°ctico para exponer temas en clase o comunidad           | Explicar el ciclo ML con ejemplos hoteleros y slides visuales         |




---

En un proyecto de Machine Learning en OCI (o en cualquier entorno cloud moderno), los actores o perfiles cumplen roles complementarios que permiten llevar una soluci√≥n desde la idea hasta el despliegue y mantenimiento. Aqu√≠ te dejo una clasificaci√≥n clara y modular que pod√©s usar como base para documentar o planificar tus proyectos:

---

## üßë‚Äçüíº Actores en un Proyecto de Machine Learning

| Actor / Perfil | Rol principal | Funcionalidades clave |
|----------------|---------------|------------------------|
| **Usuario Visual / Analista** | Interacci√≥n con resultados y dashboards | - Usa la **OCI Console** para explorar notebooks y visualizar m√©tricas<br>- Interpreta salidas del modelo para decisiones de negocio<br>- No programa, pero necesita claridad en los resultados |
| **Cient√≠fico de Datos** | Desarrollo y entrenamiento de modelos | - Crea notebooks y scripts en Python<br>- Usa **SDKs** para lanzar Jobs, entrenar y evaluar modelos<br>- Experimenta con algoritmos y limpieza de datos<br>- Documenta y versiona experimentos |
| **Desarrollador / DevOps** | Integraci√≥n y automatizaci√≥n | - Usa **SDKs**, **REST API** y **CLI** para automatizar flujos<br>- Despliega modelos como endpoints<br>- Configura pipelines y triggers<br>- Gestiona recursos y seguridad |
| **Integrador / Arquitecto de Soluciones** | Conexi√≥n entre sistemas y servicios | - Dise√±a c√≥mo el modelo se conecta con otros sistemas (ERP, CRM, etc.)<br>- Usa **REST API** para integrar el modelo en apps externas<br>- Define arquitectura en OCI (compartimentos, redes, seguridad) |
| **Administrador de Infraestructura / Cloud Engineer** | Gesti√≥n de recursos y costos | - Configura entornos, compartimentos, pol√≠ticas IAM<br>- Monitorea uso de recursos y optimiza costos<br>- Asegura cumplimiento y escalabilidad |
| **Stakeholder / Product Owner** | Visi√≥n estrat√©gica y validaci√≥n | - Define objetivos del modelo<br>- Eval√∫a impacto en negocio<br>- Participa en validaci√≥n de resultados<br>- No t√©cnico, pero clave en decisiones |

---

## üß† Ejemplo de colaboraci√≥n

Imagin√° que est√°s desarrollando un modelo de predicci√≥n de demanda:

- El **Cient√≠fico de Datos** entrena el modelo con datos hist√≥ricos.
- El **Desarrollador** crea un pipeline que lanza el Job cada semana.
- El **Integrador** conecta el modelo con el sistema de inventario.
- El **Usuario Visual** revisa los resultados en un dashboard.
- El **Administrador** asegura que todo corra en un entorno seguro y eficiente.
- El **Stakeholder** valida si el modelo mejora la planificaci√≥n.

---

